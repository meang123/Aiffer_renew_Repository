{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "302ccfe0",
   "metadata": {},
   "source": [
    "# 당뇨병 분석 \n",
    "\n",
    "사이킷런에서 제공하는 라이브러리 사용하기 전에 선형 함수 모델을 직접 구현해보고 loss function, gradient 방식으로 모델 학습을 진행 해보고 난 이후 사이킷런에서 제공하는 회귀 모델을 적용해보겠다 \n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "학습한다는 의미는 최적의 매개변수 혹은 파라미터를 찾아가는것이다 (가중치 업데이트 통해서이루어진다)\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "당뇨병의 feature name 개수 만큼 가중치 벡터를 만들어야 하고 biase 상수는 한개 정해준다 \n",
    "\n",
    "\n",
    "다변수 선형 회귀 같은 경우 w개수만큼 편미분 진행해야 한다 MSE를 loss로 사용한다면 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fc7f32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "de0ea007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형 함수 모델 정의 \n",
    "\n",
    "# 단항 선형 회귀 \n",
    "def Model(x,w,b):\n",
    "    y=w*x+b\n",
    "    return y\n",
    "\n",
    "\n",
    "# diabest 선형 회귀\n",
    "# 각 모델의 feature들로 당뇨병 수치인지 분석한다 \n",
    "def Model_diabetes(x,w,b):\n",
    "    predict_val = 0\n",
    "    for i in range(len(w)):\n",
    "        \n",
    "        predict_val+= x[:,i]*w[i]\n",
    "    \n",
    "    predict_val+=b\n",
    "    return predict_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cab7ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE 정의하기 \n",
    "def MSE(a,b):\n",
    "    return ((a - b)**2).mean()\n",
    "\n",
    "\n",
    "# RMSE 정의하기 \n",
    "def RMSE(a,b):\n",
    "    mse = MSE(a,b)\n",
    "    \n",
    "    return mse**0.5 # square\n",
    "\n",
    "# Loss function 정의 하기 \n",
    "def loss_function(x,w,b,y):\n",
    "    pred = Model_diabetes(x,w,b)\n",
    "    L = RMSE(pred,y) # pred와 실제 label과의 RMSE \n",
    "    return L \n",
    "\n",
    "\n",
    "# Gradient descent  for 단항 선형 회귀 \n",
    "def gradient(x, w, b, y):\n",
    "    dw = (loss(x, w + 0.0001, b, y) - loss(x, w, b, y)) / 0.0001\n",
    "    db = (loss(x, w, b + 0.0001, y) - loss(x, w, b, y)) / 0.0001\n",
    "    return dw, db\n",
    "\n",
    "\n",
    "def gradient_diabetes(X, W, b, y):\n",
    "    # N은 가중치의 개수\n",
    "    N = len(W)\n",
    "    \n",
    "    # y_pred 준비\n",
    "    y_pred = Model_diabetes(X, W, b)\n",
    "    \n",
    "    # 공식에 맞게 gradient 계산\n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "        \n",
    "    # b의 gradient 계산\n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ba7031d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47030135 0.51440363 0.56518611 0.71710059 0.89709003 0.05876762\n",
      " 0.56039225 0.99385061 0.2687597  0.55050739]\n",
      "0.34811292035449726\n"
     ]
    }
   ],
   "source": [
    "# initialize hyperparameter \n",
    "LEARNING_RATE = 0.01\n",
    "w = np.random.rand(10) # diabetes feature 개수는 10개이다 \n",
    "b = np.random.rand()\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4fa029d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X,y= load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = Model_diabetes(X_train,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ca9f199a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 146.4819\n",
      "Iteration 20 : Loss 126.3754\n",
      "Iteration 30 : Loss 110.7168\n",
      "Iteration 40 : Loss 98.6637\n",
      "Iteration 50 : Loss 89.4920\n",
      "Iteration 60 : Loss 82.5812\n",
      "Iteration 70 : Loss 77.4088\n",
      "Iteration 80 : Loss 73.5466\n",
      "Iteration 90 : Loss 70.6546\n",
      "Iteration 100 : Loss 68.4712\n",
      "Iteration 110 : Loss 66.8009\n",
      "Iteration 120 : Loss 65.5006\n",
      "Iteration 130 : Loss 64.4677\n",
      "Iteration 140 : Loss 63.6287\n",
      "Iteration 150 : Loss 62.9322\n",
      "Iteration 160 : Loss 62.3415\n",
      "Iteration 170 : Loss 61.8310\n",
      "Iteration 180 : Loss 61.3824\n",
      "Iteration 190 : Loss 60.9825\n",
      "Iteration 200 : Loss 60.6218\n",
      "Iteration 210 : Loss 60.2935\n",
      "Iteration 220 : Loss 59.9923\n",
      "Iteration 230 : Loss 59.7141\n",
      "Iteration 240 : Loss 59.4560\n",
      "Iteration 250 : Loss 59.2154\n",
      "Iteration 260 : Loss 58.9903\n",
      "Iteration 270 : Loss 58.7792\n",
      "Iteration 280 : Loss 58.5807\n",
      "Iteration 290 : Loss 58.3935\n",
      "Iteration 300 : Loss 58.2168\n",
      "Iteration 310 : Loss 58.0495\n",
      "Iteration 320 : Loss 57.8909\n",
      "Iteration 330 : Loss 57.7404\n",
      "Iteration 340 : Loss 57.5973\n",
      "Iteration 350 : Loss 57.4611\n",
      "Iteration 360 : Loss 57.3314\n",
      "Iteration 370 : Loss 57.2075\n",
      "Iteration 380 : Loss 57.0893\n",
      "Iteration 390 : Loss 56.9762\n",
      "Iteration 400 : Loss 56.8681\n",
      "Iteration 410 : Loss 56.7645\n",
      "Iteration 420 : Loss 56.6652\n",
      "Iteration 430 : Loss 56.5700\n",
      "Iteration 440 : Loss 56.4786\n",
      "Iteration 450 : Loss 56.3908\n",
      "Iteration 460 : Loss 56.3065\n",
      "Iteration 470 : Loss 56.2254\n",
      "Iteration 480 : Loss 56.1474\n",
      "Iteration 490 : Loss 56.0723\n",
      "Iteration 500 : Loss 56.0000\n",
      "Iteration 510 : Loss 55.9304\n",
      "Iteration 520 : Loss 55.8633\n",
      "Iteration 530 : Loss 55.7986\n",
      "Iteration 540 : Loss 55.7362\n",
      "Iteration 550 : Loss 55.6760\n",
      "Iteration 560 : Loss 55.6180\n",
      "Iteration 570 : Loss 55.5620\n",
      "Iteration 580 : Loss 55.5079\n",
      "Iteration 590 : Loss 55.4557\n",
      "Iteration 600 : Loss 55.4052\n",
      "Iteration 610 : Loss 55.3565\n",
      "Iteration 620 : Loss 55.3094\n",
      "Iteration 630 : Loss 55.2640\n",
      "Iteration 640 : Loss 55.2200\n",
      "Iteration 650 : Loss 55.1775\n",
      "Iteration 660 : Loss 55.1365\n",
      "Iteration 670 : Loss 55.0967\n",
      "Iteration 680 : Loss 55.0583\n",
      "Iteration 690 : Loss 55.0212\n",
      "Iteration 700 : Loss 54.9853\n",
      "Iteration 710 : Loss 54.9505\n",
      "Iteration 720 : Loss 54.9169\n",
      "Iteration 730 : Loss 54.8844\n",
      "Iteration 740 : Loss 54.8529\n",
      "Iteration 750 : Loss 54.8225\n",
      "Iteration 760 : Loss 54.7930\n",
      "Iteration 770 : Loss 54.7645\n",
      "Iteration 780 : Loss 54.7369\n",
      "Iteration 790 : Loss 54.7101\n",
      "Iteration 800 : Loss 54.6843\n",
      "Iteration 810 : Loss 54.6592\n",
      "Iteration 820 : Loss 54.6350\n",
      "Iteration 830 : Loss 54.6115\n",
      "Iteration 840 : Loss 54.5887\n",
      "Iteration 850 : Loss 54.5667\n",
      "Iteration 860 : Loss 54.5454\n",
      "Iteration 870 : Loss 54.5247\n",
      "Iteration 880 : Loss 54.5047\n",
      "Iteration 890 : Loss 54.4853\n",
      "Iteration 900 : Loss 54.4665\n",
      "Iteration 910 : Loss 54.4483\n",
      "Iteration 920 : Loss 54.4306\n",
      "Iteration 930 : Loss 54.4136\n",
      "Iteration 940 : Loss 54.3970\n",
      "Iteration 950 : Loss 54.3809\n",
      "Iteration 960 : Loss 54.3654\n",
      "Iteration 970 : Loss 54.3503\n",
      "Iteration 980 : Loss 54.3357\n",
      "Iteration 990 : Loss 54.3215\n",
      "Iteration 1000 : Loss 54.3078\n",
      "Iteration 1010 : Loss 54.2945\n",
      "Iteration 1020 : Loss 54.2816\n",
      "Iteration 1030 : Loss 54.2691\n",
      "Iteration 1040 : Loss 54.2569\n",
      "Iteration 1050 : Loss 54.2451\n",
      "Iteration 1060 : Loss 54.2337\n",
      "Iteration 1070 : Loss 54.2226\n",
      "Iteration 1080 : Loss 54.2119\n",
      "Iteration 1090 : Loss 54.2015\n",
      "Iteration 1100 : Loss 54.1914\n",
      "Iteration 1110 : Loss 54.1816\n",
      "Iteration 1120 : Loss 54.1721\n",
      "Iteration 1130 : Loss 54.1628\n",
      "Iteration 1140 : Loss 54.1539\n",
      "Iteration 1150 : Loss 54.1452\n",
      "Iteration 1160 : Loss 54.1367\n",
      "Iteration 1170 : Loss 54.1286\n",
      "Iteration 1180 : Loss 54.1206\n",
      "Iteration 1190 : Loss 54.1129\n",
      "Iteration 1200 : Loss 54.1054\n",
      "Iteration 1210 : Loss 54.0981\n",
      "Iteration 1220 : Loss 54.0911\n",
      "Iteration 1230 : Loss 54.0842\n",
      "Iteration 1240 : Loss 54.0776\n",
      "Iteration 1250 : Loss 54.0711\n",
      "Iteration 1260 : Loss 54.0648\n",
      "Iteration 1270 : Loss 54.0587\n",
      "Iteration 1280 : Loss 54.0528\n",
      "Iteration 1290 : Loss 54.0471\n",
      "Iteration 1300 : Loss 54.0415\n",
      "Iteration 1310 : Loss 54.0361\n",
      "Iteration 1320 : Loss 54.0308\n",
      "Iteration 1330 : Loss 54.0257\n",
      "Iteration 1340 : Loss 54.0207\n",
      "Iteration 1350 : Loss 54.0158\n",
      "Iteration 1360 : Loss 54.0111\n",
      "Iteration 1370 : Loss 54.0066\n",
      "Iteration 1380 : Loss 54.0021\n",
      "Iteration 1390 : Loss 53.9978\n",
      "Iteration 1400 : Loss 53.9936\n",
      "Iteration 1410 : Loss 53.9895\n",
      "Iteration 1420 : Loss 53.9855\n",
      "Iteration 1430 : Loss 53.9817\n",
      "Iteration 1440 : Loss 53.9779\n",
      "Iteration 1450 : Loss 53.9743\n",
      "Iteration 1460 : Loss 53.9707\n",
      "Iteration 1470 : Loss 53.9672\n",
      "Iteration 1480 : Loss 53.9639\n",
      "Iteration 1490 : Loss 53.9606\n",
      "Iteration 1500 : Loss 53.9574\n",
      "Iteration 1510 : Loss 53.9543\n",
      "Iteration 1520 : Loss 53.9513\n",
      "Iteration 1530 : Loss 53.9484\n",
      "Iteration 1540 : Loss 53.9455\n",
      "Iteration 1550 : Loss 53.9428\n",
      "Iteration 1560 : Loss 53.9400\n",
      "Iteration 1570 : Loss 53.9374\n",
      "Iteration 1580 : Loss 53.9348\n",
      "Iteration 1590 : Loss 53.9324\n",
      "Iteration 1600 : Loss 53.9299\n",
      "Iteration 1610 : Loss 53.9276\n",
      "Iteration 1620 : Loss 53.9252\n",
      "Iteration 1630 : Loss 53.9230\n",
      "Iteration 1640 : Loss 53.9208\n",
      "Iteration 1650 : Loss 53.9187\n",
      "Iteration 1660 : Loss 53.9166\n",
      "Iteration 1670 : Loss 53.9146\n",
      "Iteration 1680 : Loss 53.9126\n",
      "Iteration 1690 : Loss 53.9107\n",
      "Iteration 1700 : Loss 53.9088\n",
      "Iteration 1710 : Loss 53.9070\n",
      "Iteration 1720 : Loss 53.9052\n",
      "Iteration 1730 : Loss 53.9035\n",
      "Iteration 1740 : Loss 53.9018\n",
      "Iteration 1750 : Loss 53.9001\n",
      "Iteration 1760 : Loss 53.8985\n",
      "Iteration 1770 : Loss 53.8969\n",
      "Iteration 1780 : Loss 53.8954\n",
      "Iteration 1790 : Loss 53.8939\n",
      "Iteration 1800 : Loss 53.8925\n",
      "Iteration 1810 : Loss 53.8910\n",
      "Iteration 1820 : Loss 53.8897\n",
      "Iteration 1830 : Loss 53.8883\n",
      "Iteration 1840 : Loss 53.8870\n",
      "Iteration 1850 : Loss 53.8857\n",
      "Iteration 1860 : Loss 53.8844\n",
      "Iteration 1870 : Loss 53.8832\n",
      "Iteration 1880 : Loss 53.8820\n",
      "Iteration 1890 : Loss 53.8808\n",
      "Iteration 1900 : Loss 53.8797\n",
      "Iteration 1910 : Loss 53.8786\n",
      "Iteration 1920 : Loss 53.8775\n",
      "Iteration 1930 : Loss 53.8764\n",
      "Iteration 1940 : Loss 53.8754\n",
      "Iteration 1950 : Loss 53.8744\n",
      "Iteration 1960 : Loss 53.8734\n",
      "Iteration 1970 : Loss 53.8724\n",
      "Iteration 1980 : Loss 53.8715\n",
      "Iteration 1990 : Loss 53.8705\n",
      "Iteration 2000 : Loss 53.8696\n",
      "Iteration 2010 : Loss 53.8687\n",
      "Iteration 2020 : Loss 53.8679\n",
      "Iteration 2030 : Loss 53.8670\n",
      "Iteration 2040 : Loss 53.8662\n",
      "Iteration 2050 : Loss 53.8654\n",
      "Iteration 2060 : Loss 53.8646\n",
      "Iteration 2070 : Loss 53.8638\n",
      "Iteration 2080 : Loss 53.8631\n",
      "Iteration 2090 : Loss 53.8623\n",
      "Iteration 2100 : Loss 53.8616\n",
      "Iteration 2110 : Loss 53.8609\n",
      "Iteration 2120 : Loss 53.8602\n",
      "Iteration 2130 : Loss 53.8595\n",
      "Iteration 2140 : Loss 53.8589\n",
      "Iteration 2150 : Loss 53.8582\n",
      "Iteration 2160 : Loss 53.8576\n",
      "Iteration 2170 : Loss 53.8569\n",
      "Iteration 2180 : Loss 53.8563\n",
      "Iteration 2190 : Loss 53.8557\n",
      "Iteration 2200 : Loss 53.8551\n",
      "Iteration 2210 : Loss 53.8546\n",
      "Iteration 2220 : Loss 53.8540\n",
      "Iteration 2230 : Loss 53.8535\n",
      "Iteration 2240 : Loss 53.8529\n",
      "Iteration 2250 : Loss 53.8524\n",
      "Iteration 2260 : Loss 53.8519\n",
      "Iteration 2270 : Loss 53.8514\n",
      "Iteration 2280 : Loss 53.8509\n",
      "Iteration 2290 : Loss 53.8504\n",
      "Iteration 2300 : Loss 53.8499\n",
      "Iteration 2310 : Loss 53.8494\n",
      "Iteration 2320 : Loss 53.8490\n",
      "Iteration 2330 : Loss 53.8485\n",
      "Iteration 2340 : Loss 53.8481\n",
      "Iteration 2350 : Loss 53.8476\n",
      "Iteration 2360 : Loss 53.8472\n",
      "Iteration 2370 : Loss 53.8468\n",
      "Iteration 2380 : Loss 53.8464\n",
      "Iteration 2390 : Loss 53.8460\n",
      "Iteration 2400 : Loss 53.8456\n",
      "Iteration 2410 : Loss 53.8452\n",
      "Iteration 2420 : Loss 53.8448\n",
      "Iteration 2430 : Loss 53.8444\n",
      "Iteration 2440 : Loss 53.8440\n",
      "Iteration 2450 : Loss 53.8437\n",
      "Iteration 2460 : Loss 53.8433\n",
      "Iteration 2470 : Loss 53.8430\n",
      "Iteration 2480 : Loss 53.8426\n",
      "Iteration 2490 : Loss 53.8423\n",
      "Iteration 2500 : Loss 53.8420\n",
      "Iteration 2510 : Loss 53.8416\n",
      "Iteration 2520 : Loss 53.8413\n",
      "Iteration 2530 : Loss 53.8410\n",
      "Iteration 2540 : Loss 53.8407\n",
      "Iteration 2550 : Loss 53.8404\n",
      "Iteration 2560 : Loss 53.8401\n",
      "Iteration 2570 : Loss 53.8398\n",
      "Iteration 2580 : Loss 53.8395\n",
      "Iteration 2590 : Loss 53.8392\n",
      "Iteration 2600 : Loss 53.8389\n",
      "Iteration 2610 : Loss 53.8386\n",
      "Iteration 2620 : Loss 53.8384\n",
      "Iteration 2630 : Loss 53.8381\n",
      "Iteration 2640 : Loss 53.8378\n",
      "Iteration 2650 : Loss 53.8376\n",
      "Iteration 2660 : Loss 53.8373\n",
      "Iteration 2670 : Loss 53.8371\n",
      "Iteration 2680 : Loss 53.8368\n",
      "Iteration 2690 : Loss 53.8366\n",
      "Iteration 2700 : Loss 53.8363\n",
      "Iteration 2710 : Loss 53.8361\n",
      "Iteration 2720 : Loss 53.8359\n",
      "Iteration 2730 : Loss 53.8356\n",
      "Iteration 2740 : Loss 53.8354\n",
      "Iteration 2750 : Loss 53.8352\n",
      "Iteration 2760 : Loss 53.8350\n",
      "Iteration 2770 : Loss 53.8347\n",
      "Iteration 2780 : Loss 53.8345\n",
      "Iteration 2790 : Loss 53.8343\n",
      "Iteration 2800 : Loss 53.8341\n",
      "Iteration 2810 : Loss 53.8339\n",
      "Iteration 2820 : Loss 53.8337\n",
      "Iteration 2830 : Loss 53.8335\n",
      "Iteration 2840 : Loss 53.8333\n",
      "Iteration 2850 : Loss 53.8331\n",
      "Iteration 2860 : Loss 53.8329\n",
      "Iteration 2870 : Loss 53.8327\n",
      "Iteration 2880 : Loss 53.8325\n",
      "Iteration 2890 : Loss 53.8323\n",
      "Iteration 2900 : Loss 53.8321\n",
      "Iteration 2910 : Loss 53.8319\n",
      "Iteration 2920 : Loss 53.8318\n",
      "Iteration 2930 : Loss 53.8316\n",
      "Iteration 2940 : Loss 53.8314\n",
      "Iteration 2950 : Loss 53.8312\n",
      "Iteration 2960 : Loss 53.8311\n",
      "Iteration 2970 : Loss 53.8309\n",
      "Iteration 2980 : Loss 53.8307\n",
      "Iteration 2990 : Loss 53.8306\n",
      "Iteration 3000 : Loss 53.8304\n",
      "Iteration 3010 : Loss 53.8302\n",
      "Iteration 3020 : Loss 53.8301\n",
      "Iteration 3030 : Loss 53.8299\n",
      "Iteration 3040 : Loss 53.8298\n",
      "Iteration 3050 : Loss 53.8296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3060 : Loss 53.8294\n",
      "Iteration 3070 : Loss 53.8293\n",
      "Iteration 3080 : Loss 53.8291\n",
      "Iteration 3090 : Loss 53.8290\n",
      "Iteration 3100 : Loss 53.8288\n",
      "Iteration 3110 : Loss 53.8287\n",
      "Iteration 3120 : Loss 53.8285\n",
      "Iteration 3130 : Loss 53.8284\n",
      "Iteration 3140 : Loss 53.8283\n",
      "Iteration 3150 : Loss 53.8281\n",
      "Iteration 3160 : Loss 53.8280\n",
      "Iteration 3170 : Loss 53.8278\n",
      "Iteration 3180 : Loss 53.8277\n",
      "Iteration 3190 : Loss 53.8276\n",
      "Iteration 3200 : Loss 53.8274\n",
      "Iteration 3210 : Loss 53.8273\n",
      "Iteration 3220 : Loss 53.8272\n",
      "Iteration 3230 : Loss 53.8270\n",
      "Iteration 3240 : Loss 53.8269\n",
      "Iteration 3250 : Loss 53.8268\n",
      "Iteration 3260 : Loss 53.8266\n",
      "Iteration 3270 : Loss 53.8265\n",
      "Iteration 3280 : Loss 53.8264\n",
      "Iteration 3290 : Loss 53.8262\n",
      "Iteration 3300 : Loss 53.8261\n",
      "Iteration 3310 : Loss 53.8260\n",
      "Iteration 3320 : Loss 53.8259\n",
      "Iteration 3330 : Loss 53.8258\n",
      "Iteration 3340 : Loss 53.8256\n",
      "Iteration 3350 : Loss 53.8255\n",
      "Iteration 3360 : Loss 53.8254\n",
      "Iteration 3370 : Loss 53.8253\n",
      "Iteration 3380 : Loss 53.8252\n",
      "Iteration 3390 : Loss 53.8250\n",
      "Iteration 3400 : Loss 53.8249\n",
      "Iteration 3410 : Loss 53.8248\n",
      "Iteration 3420 : Loss 53.8247\n",
      "Iteration 3430 : Loss 53.8246\n",
      "Iteration 3440 : Loss 53.8245\n",
      "Iteration 3450 : Loss 53.8244\n",
      "Iteration 3460 : Loss 53.8242\n",
      "Iteration 3470 : Loss 53.8241\n",
      "Iteration 3480 : Loss 53.8240\n",
      "Iteration 3490 : Loss 53.8239\n",
      "Iteration 3500 : Loss 53.8238\n",
      "Iteration 3510 : Loss 53.8237\n",
      "Iteration 3520 : Loss 53.8236\n",
      "Iteration 3530 : Loss 53.8235\n",
      "Iteration 3540 : Loss 53.8234\n",
      "Iteration 3550 : Loss 53.8233\n",
      "Iteration 3560 : Loss 53.8232\n",
      "Iteration 3570 : Loss 53.8230\n",
      "Iteration 3580 : Loss 53.8229\n",
      "Iteration 3590 : Loss 53.8228\n",
      "Iteration 3600 : Loss 53.8227\n",
      "Iteration 3610 : Loss 53.8226\n",
      "Iteration 3620 : Loss 53.8225\n",
      "Iteration 3630 : Loss 53.8224\n",
      "Iteration 3640 : Loss 53.8223\n",
      "Iteration 3650 : Loss 53.8222\n",
      "Iteration 3660 : Loss 53.8221\n",
      "Iteration 3670 : Loss 53.8220\n",
      "Iteration 3680 : Loss 53.8219\n",
      "Iteration 3690 : Loss 53.8218\n",
      "Iteration 3700 : Loss 53.8217\n",
      "Iteration 3710 : Loss 53.8216\n",
      "Iteration 3720 : Loss 53.8215\n",
      "Iteration 3730 : Loss 53.8214\n",
      "Iteration 3740 : Loss 53.8213\n",
      "Iteration 3750 : Loss 53.8212\n",
      "Iteration 3760 : Loss 53.8211\n",
      "Iteration 3770 : Loss 53.8210\n",
      "Iteration 3780 : Loss 53.8210\n",
      "Iteration 3790 : Loss 53.8209\n",
      "Iteration 3800 : Loss 53.8208\n",
      "Iteration 3810 : Loss 53.8207\n",
      "Iteration 3820 : Loss 53.8206\n",
      "Iteration 3830 : Loss 53.8205\n",
      "Iteration 3840 : Loss 53.8204\n",
      "Iteration 3850 : Loss 53.8203\n",
      "Iteration 3860 : Loss 53.8202\n",
      "Iteration 3870 : Loss 53.8201\n",
      "Iteration 3880 : Loss 53.8200\n",
      "Iteration 3890 : Loss 53.8199\n",
      "Iteration 3900 : Loss 53.8198\n",
      "Iteration 3910 : Loss 53.8197\n",
      "Iteration 3920 : Loss 53.8197\n",
      "Iteration 3930 : Loss 53.8196\n",
      "Iteration 3940 : Loss 53.8195\n",
      "Iteration 3950 : Loss 53.8194\n",
      "Iteration 3960 : Loss 53.8193\n",
      "Iteration 3970 : Loss 53.8192\n",
      "Iteration 3980 : Loss 53.8191\n",
      "Iteration 3990 : Loss 53.8190\n",
      "Iteration 4000 : Loss 53.8189\n",
      "Iteration 4010 : Loss 53.8188\n",
      "Iteration 4020 : Loss 53.8188\n",
      "Iteration 4030 : Loss 53.8187\n",
      "Iteration 4040 : Loss 53.8186\n",
      "Iteration 4050 : Loss 53.8185\n",
      "Iteration 4060 : Loss 53.8184\n",
      "Iteration 4070 : Loss 53.8183\n",
      "Iteration 4080 : Loss 53.8182\n",
      "Iteration 4090 : Loss 53.8181\n",
      "Iteration 4100 : Loss 53.8181\n",
      "Iteration 4110 : Loss 53.8180\n",
      "Iteration 4120 : Loss 53.8179\n",
      "Iteration 4130 : Loss 53.8178\n",
      "Iteration 4140 : Loss 53.8177\n",
      "Iteration 4150 : Loss 53.8176\n",
      "Iteration 4160 : Loss 53.8175\n",
      "Iteration 4170 : Loss 53.8175\n",
      "Iteration 4180 : Loss 53.8174\n",
      "Iteration 4190 : Loss 53.8173\n",
      "Iteration 4200 : Loss 53.8172\n",
      "Iteration 4210 : Loss 53.8171\n",
      "Iteration 4220 : Loss 53.8170\n",
      "Iteration 4230 : Loss 53.8169\n",
      "Iteration 4240 : Loss 53.8169\n",
      "Iteration 4250 : Loss 53.8168\n",
      "Iteration 4260 : Loss 53.8167\n",
      "Iteration 4270 : Loss 53.8166\n",
      "Iteration 4280 : Loss 53.8165\n",
      "Iteration 4290 : Loss 53.8164\n",
      "Iteration 4300 : Loss 53.8164\n",
      "Iteration 4310 : Loss 53.8163\n",
      "Iteration 4320 : Loss 53.8162\n",
      "Iteration 4330 : Loss 53.8161\n",
      "Iteration 4340 : Loss 53.8160\n",
      "Iteration 4350 : Loss 53.8160\n",
      "Iteration 4360 : Loss 53.8159\n",
      "Iteration 4370 : Loss 53.8158\n",
      "Iteration 4380 : Loss 53.8157\n",
      "Iteration 4390 : Loss 53.8156\n",
      "Iteration 4400 : Loss 53.8155\n",
      "Iteration 4410 : Loss 53.8155\n",
      "Iteration 4420 : Loss 53.8154\n",
      "Iteration 4430 : Loss 53.8153\n",
      "Iteration 4440 : Loss 53.8152\n",
      "Iteration 4450 : Loss 53.8151\n",
      "Iteration 4460 : Loss 53.8151\n",
      "Iteration 4470 : Loss 53.8150\n",
      "Iteration 4480 : Loss 53.8149\n",
      "Iteration 4490 : Loss 53.8148\n",
      "Iteration 4500 : Loss 53.8147\n",
      "Iteration 4510 : Loss 53.8147\n",
      "Iteration 4520 : Loss 53.8146\n",
      "Iteration 4530 : Loss 53.8145\n",
      "Iteration 4540 : Loss 53.8144\n",
      "Iteration 4550 : Loss 53.8143\n",
      "Iteration 4560 : Loss 53.8143\n",
      "Iteration 4570 : Loss 53.8142\n",
      "Iteration 4580 : Loss 53.8141\n",
      "Iteration 4590 : Loss 53.8140\n",
      "Iteration 4600 : Loss 53.8139\n",
      "Iteration 4610 : Loss 53.8139\n",
      "Iteration 4620 : Loss 53.8138\n",
      "Iteration 4630 : Loss 53.8137\n",
      "Iteration 4640 : Loss 53.8136\n",
      "Iteration 4650 : Loss 53.8135\n",
      "Iteration 4660 : Loss 53.8135\n",
      "Iteration 4670 : Loss 53.8134\n",
      "Iteration 4680 : Loss 53.8133\n",
      "Iteration 4690 : Loss 53.8132\n",
      "Iteration 4700 : Loss 53.8131\n",
      "Iteration 4710 : Loss 53.8131\n",
      "Iteration 4720 : Loss 53.8130\n",
      "Iteration 4730 : Loss 53.8129\n",
      "Iteration 4740 : Loss 53.8128\n",
      "Iteration 4750 : Loss 53.8128\n",
      "Iteration 4760 : Loss 53.8127\n",
      "Iteration 4770 : Loss 53.8126\n",
      "Iteration 4780 : Loss 53.8125\n",
      "Iteration 4790 : Loss 53.8124\n",
      "Iteration 4800 : Loss 53.8124\n",
      "Iteration 4810 : Loss 53.8123\n",
      "Iteration 4820 : Loss 53.8122\n",
      "Iteration 4830 : Loss 53.8121\n",
      "Iteration 4840 : Loss 53.8121\n",
      "Iteration 4850 : Loss 53.8120\n",
      "Iteration 4860 : Loss 53.8119\n",
      "Iteration 4870 : Loss 53.8118\n",
      "Iteration 4880 : Loss 53.8117\n",
      "Iteration 4890 : Loss 53.8117\n",
      "Iteration 4900 : Loss 53.8116\n",
      "Iteration 4910 : Loss 53.8115\n",
      "Iteration 4920 : Loss 53.8114\n",
      "Iteration 4930 : Loss 53.8114\n",
      "Iteration 4940 : Loss 53.8113\n",
      "Iteration 4950 : Loss 53.8112\n",
      "Iteration 4960 : Loss 53.8111\n",
      "Iteration 4970 : Loss 53.8111\n",
      "Iteration 4980 : Loss 53.8110\n",
      "Iteration 4990 : Loss 53.8109\n",
      "Iteration 5000 : Loss 53.8108\n",
      "Iteration 5010 : Loss 53.8107\n",
      "Iteration 5020 : Loss 53.8107\n",
      "Iteration 5030 : Loss 53.8106\n",
      "Iteration 5040 : Loss 53.8105\n",
      "Iteration 5050 : Loss 53.8104\n",
      "Iteration 5060 : Loss 53.8104\n",
      "Iteration 5070 : Loss 53.8103\n",
      "Iteration 5080 : Loss 53.8102\n",
      "Iteration 5090 : Loss 53.8101\n",
      "Iteration 5100 : Loss 53.8101\n",
      "Iteration 5110 : Loss 53.8100\n",
      "Iteration 5120 : Loss 53.8099\n",
      "Iteration 5130 : Loss 53.8098\n",
      "Iteration 5140 : Loss 53.8098\n",
      "Iteration 5150 : Loss 53.8097\n",
      "Iteration 5160 : Loss 53.8096\n",
      "Iteration 5170 : Loss 53.8095\n",
      "Iteration 5180 : Loss 53.8095\n",
      "Iteration 5190 : Loss 53.8094\n",
      "Iteration 5200 : Loss 53.8093\n",
      "Iteration 5210 : Loss 53.8092\n",
      "Iteration 5220 : Loss 53.8092\n",
      "Iteration 5230 : Loss 53.8091\n",
      "Iteration 5240 : Loss 53.8090\n",
      "Iteration 5250 : Loss 53.8089\n",
      "Iteration 5260 : Loss 53.8089\n",
      "Iteration 5270 : Loss 53.8088\n",
      "Iteration 5280 : Loss 53.8087\n",
      "Iteration 5290 : Loss 53.8086\n",
      "Iteration 5300 : Loss 53.8086\n",
      "Iteration 5310 : Loss 53.8085\n",
      "Iteration 5320 : Loss 53.8084\n",
      "Iteration 5330 : Loss 53.8083\n",
      "Iteration 5340 : Loss 53.8083\n",
      "Iteration 5350 : Loss 53.8082\n",
      "Iteration 5360 : Loss 53.8081\n",
      "Iteration 5370 : Loss 53.8080\n",
      "Iteration 5380 : Loss 53.8080\n",
      "Iteration 5390 : Loss 53.8079\n",
      "Iteration 5400 : Loss 53.8078\n",
      "Iteration 5410 : Loss 53.8077\n",
      "Iteration 5420 : Loss 53.8077\n",
      "Iteration 5430 : Loss 53.8076\n",
      "Iteration 5440 : Loss 53.8075\n",
      "Iteration 5450 : Loss 53.8074\n",
      "Iteration 5460 : Loss 53.8074\n",
      "Iteration 5470 : Loss 53.8073\n",
      "Iteration 5480 : Loss 53.8072\n",
      "Iteration 5490 : Loss 53.8071\n",
      "Iteration 5500 : Loss 53.8071\n",
      "Iteration 5510 : Loss 53.8070\n",
      "Iteration 5520 : Loss 53.8069\n",
      "Iteration 5530 : Loss 53.8068\n",
      "Iteration 5540 : Loss 53.8068\n",
      "Iteration 5550 : Loss 53.8067\n",
      "Iteration 5560 : Loss 53.8066\n",
      "Iteration 5570 : Loss 53.8065\n",
      "Iteration 5580 : Loss 53.8065\n",
      "Iteration 5590 : Loss 53.8064\n",
      "Iteration 5600 : Loss 53.8063\n",
      "Iteration 5610 : Loss 53.8062\n",
      "Iteration 5620 : Loss 53.8062\n",
      "Iteration 5630 : Loss 53.8061\n",
      "Iteration 5640 : Loss 53.8060\n",
      "Iteration 5650 : Loss 53.8060\n",
      "Iteration 5660 : Loss 53.8059\n",
      "Iteration 5670 : Loss 53.8058\n",
      "Iteration 5680 : Loss 53.8057\n",
      "Iteration 5690 : Loss 53.8057\n",
      "Iteration 5700 : Loss 53.8056\n",
      "Iteration 5710 : Loss 53.8055\n",
      "Iteration 5720 : Loss 53.8054\n",
      "Iteration 5730 : Loss 53.8054\n",
      "Iteration 5740 : Loss 53.8053\n",
      "Iteration 5750 : Loss 53.8052\n",
      "Iteration 5760 : Loss 53.8051\n",
      "Iteration 5770 : Loss 53.8051\n",
      "Iteration 5780 : Loss 53.8050\n",
      "Iteration 5790 : Loss 53.8049\n",
      "Iteration 5800 : Loss 53.8049\n",
      "Iteration 5810 : Loss 53.8048\n",
      "Iteration 5820 : Loss 53.8047\n",
      "Iteration 5830 : Loss 53.8046\n",
      "Iteration 5840 : Loss 53.8046\n",
      "Iteration 5850 : Loss 53.8045\n",
      "Iteration 5860 : Loss 53.8044\n",
      "Iteration 5870 : Loss 53.8043\n",
      "Iteration 5880 : Loss 53.8043\n",
      "Iteration 5890 : Loss 53.8042\n",
      "Iteration 5900 : Loss 53.8041\n",
      "Iteration 5910 : Loss 53.8040\n",
      "Iteration 5920 : Loss 53.8040\n",
      "Iteration 5930 : Loss 53.8039\n",
      "Iteration 5940 : Loss 53.8038\n",
      "Iteration 5950 : Loss 53.8038\n",
      "Iteration 5960 : Loss 53.8037\n",
      "Iteration 5970 : Loss 53.8036\n",
      "Iteration 5980 : Loss 53.8035\n",
      "Iteration 5990 : Loss 53.8035\n",
      "Iteration 6000 : Loss 53.8034\n",
      "Iteration 6010 : Loss 53.8033\n",
      "Iteration 6020 : Loss 53.8032\n",
      "Iteration 6030 : Loss 53.8032\n",
      "Iteration 6040 : Loss 53.8031\n",
      "Iteration 6050 : Loss 53.8030\n",
      "Iteration 6060 : Loss 53.8030\n",
      "Iteration 6070 : Loss 53.8029\n",
      "Iteration 6080 : Loss 53.8028\n",
      "Iteration 6090 : Loss 53.8027\n",
      "Iteration 6100 : Loss 53.8027\n",
      "Iteration 6110 : Loss 53.8026\n",
      "Iteration 6120 : Loss 53.8025\n",
      "Iteration 6130 : Loss 53.8025\n",
      "Iteration 6140 : Loss 53.8024\n",
      "Iteration 6150 : Loss 53.8023\n",
      "Iteration 6160 : Loss 53.8022\n",
      "Iteration 6170 : Loss 53.8022\n",
      "Iteration 6180 : Loss 53.8021\n",
      "Iteration 6190 : Loss 53.8020\n",
      "Iteration 6200 : Loss 53.8019\n",
      "Iteration 6210 : Loss 53.8019\n",
      "Iteration 6220 : Loss 53.8018\n",
      "Iteration 6230 : Loss 53.8017\n",
      "Iteration 6240 : Loss 53.8017\n",
      "Iteration 6250 : Loss 53.8016\n",
      "Iteration 6260 : Loss 53.8015\n",
      "Iteration 6270 : Loss 53.8014\n",
      "Iteration 6280 : Loss 53.8014\n",
      "Iteration 6290 : Loss 53.8013\n",
      "Iteration 6300 : Loss 53.8012\n",
      "Iteration 6310 : Loss 53.8012\n",
      "Iteration 6320 : Loss 53.8011\n",
      "Iteration 6330 : Loss 53.8010\n",
      "Iteration 6340 : Loss 53.8009\n",
      "Iteration 6350 : Loss 53.8009\n",
      "Iteration 6360 : Loss 53.8008\n",
      "Iteration 6370 : Loss 53.8007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6380 : Loss 53.8007\n",
      "Iteration 6390 : Loss 53.8006\n",
      "Iteration 6400 : Loss 53.8005\n",
      "Iteration 6410 : Loss 53.8004\n",
      "Iteration 6420 : Loss 53.8004\n",
      "Iteration 6430 : Loss 53.8003\n",
      "Iteration 6440 : Loss 53.8002\n",
      "Iteration 6450 : Loss 53.8002\n",
      "Iteration 6460 : Loss 53.8001\n",
      "Iteration 6470 : Loss 53.8000\n",
      "Iteration 6480 : Loss 53.7999\n",
      "Iteration 6490 : Loss 53.7999\n",
      "Iteration 6500 : Loss 53.7998\n",
      "Iteration 6510 : Loss 53.7997\n",
      "Iteration 6520 : Loss 53.7997\n",
      "Iteration 6530 : Loss 53.7996\n",
      "Iteration 6540 : Loss 53.7995\n",
      "Iteration 6550 : Loss 53.7994\n",
      "Iteration 6560 : Loss 53.7994\n",
      "Iteration 6570 : Loss 53.7993\n",
      "Iteration 6580 : Loss 53.7992\n",
      "Iteration 6590 : Loss 53.7992\n",
      "Iteration 6600 : Loss 53.7991\n",
      "Iteration 6610 : Loss 53.7990\n",
      "Iteration 6620 : Loss 53.7989\n",
      "Iteration 6630 : Loss 53.7989\n",
      "Iteration 6640 : Loss 53.7988\n",
      "Iteration 6650 : Loss 53.7987\n",
      "Iteration 6660 : Loss 53.7987\n",
      "Iteration 6670 : Loss 53.7986\n",
      "Iteration 6680 : Loss 53.7985\n",
      "Iteration 6690 : Loss 53.7984\n",
      "Iteration 6700 : Loss 53.7984\n",
      "Iteration 6710 : Loss 53.7983\n",
      "Iteration 6720 : Loss 53.7982\n",
      "Iteration 6730 : Loss 53.7982\n",
      "Iteration 6740 : Loss 53.7981\n",
      "Iteration 6750 : Loss 53.7980\n",
      "Iteration 6760 : Loss 53.7979\n",
      "Iteration 6770 : Loss 53.7979\n",
      "Iteration 6780 : Loss 53.7978\n",
      "Iteration 6790 : Loss 53.7977\n",
      "Iteration 6800 : Loss 53.7977\n",
      "Iteration 6810 : Loss 53.7976\n",
      "Iteration 6820 : Loss 53.7975\n",
      "Iteration 6830 : Loss 53.7975\n",
      "Iteration 6840 : Loss 53.7974\n",
      "Iteration 6850 : Loss 53.7973\n",
      "Iteration 6860 : Loss 53.7972\n",
      "Iteration 6870 : Loss 53.7972\n",
      "Iteration 6880 : Loss 53.7971\n",
      "Iteration 6890 : Loss 53.7970\n",
      "Iteration 6900 : Loss 53.7970\n",
      "Iteration 6910 : Loss 53.7969\n",
      "Iteration 6920 : Loss 53.7968\n",
      "Iteration 6930 : Loss 53.7967\n",
      "Iteration 6940 : Loss 53.7967\n",
      "Iteration 6950 : Loss 53.7966\n",
      "Iteration 6960 : Loss 53.7965\n",
      "Iteration 6970 : Loss 53.7965\n",
      "Iteration 6980 : Loss 53.7964\n",
      "Iteration 6990 : Loss 53.7963\n",
      "Iteration 7000 : Loss 53.7963\n",
      "Iteration 7010 : Loss 53.7962\n",
      "Iteration 7020 : Loss 53.7961\n",
      "Iteration 7030 : Loss 53.7960\n",
      "Iteration 7040 : Loss 53.7960\n",
      "Iteration 7050 : Loss 53.7959\n",
      "Iteration 7060 : Loss 53.7958\n",
      "Iteration 7070 : Loss 53.7958\n",
      "Iteration 7080 : Loss 53.7957\n",
      "Iteration 7090 : Loss 53.7956\n",
      "Iteration 7100 : Loss 53.7956\n",
      "Iteration 7110 : Loss 53.7955\n",
      "Iteration 7120 : Loss 53.7954\n",
      "Iteration 7130 : Loss 53.7953\n",
      "Iteration 7140 : Loss 53.7953\n",
      "Iteration 7150 : Loss 53.7952\n",
      "Iteration 7160 : Loss 53.7951\n",
      "Iteration 7170 : Loss 53.7951\n",
      "Iteration 7180 : Loss 53.7950\n",
      "Iteration 7190 : Loss 53.7949\n",
      "Iteration 7200 : Loss 53.7949\n",
      "Iteration 7210 : Loss 53.7948\n",
      "Iteration 7220 : Loss 53.7947\n",
      "Iteration 7230 : Loss 53.7946\n",
      "Iteration 7240 : Loss 53.7946\n",
      "Iteration 7250 : Loss 53.7945\n",
      "Iteration 7260 : Loss 53.7944\n",
      "Iteration 7270 : Loss 53.7944\n",
      "Iteration 7280 : Loss 53.7943\n",
      "Iteration 7290 : Loss 53.7942\n",
      "Iteration 7300 : Loss 53.7942\n",
      "Iteration 7310 : Loss 53.7941\n",
      "Iteration 7320 : Loss 53.7940\n",
      "Iteration 7330 : Loss 53.7939\n",
      "Iteration 7340 : Loss 53.7939\n",
      "Iteration 7350 : Loss 53.7938\n",
      "Iteration 7360 : Loss 53.7937\n",
      "Iteration 7370 : Loss 53.7937\n",
      "Iteration 7380 : Loss 53.7936\n",
      "Iteration 7390 : Loss 53.7935\n",
      "Iteration 7400 : Loss 53.7935\n",
      "Iteration 7410 : Loss 53.7934\n",
      "Iteration 7420 : Loss 53.7933\n",
      "Iteration 7430 : Loss 53.7933\n",
      "Iteration 7440 : Loss 53.7932\n",
      "Iteration 7450 : Loss 53.7931\n",
      "Iteration 7460 : Loss 53.7930\n",
      "Iteration 7470 : Loss 53.7930\n",
      "Iteration 7480 : Loss 53.7929\n",
      "Iteration 7490 : Loss 53.7928\n",
      "Iteration 7500 : Loss 53.7928\n",
      "Iteration 7510 : Loss 53.7927\n",
      "Iteration 7520 : Loss 53.7926\n",
      "Iteration 7530 : Loss 53.7926\n",
      "Iteration 7540 : Loss 53.7925\n",
      "Iteration 7550 : Loss 53.7924\n",
      "Iteration 7560 : Loss 53.7924\n",
      "Iteration 7570 : Loss 53.7923\n",
      "Iteration 7580 : Loss 53.7922\n",
      "Iteration 7590 : Loss 53.7921\n",
      "Iteration 7600 : Loss 53.7921\n",
      "Iteration 7610 : Loss 53.7920\n",
      "Iteration 7620 : Loss 53.7919\n",
      "Iteration 7630 : Loss 53.7919\n",
      "Iteration 7640 : Loss 53.7918\n",
      "Iteration 7650 : Loss 53.7917\n",
      "Iteration 7660 : Loss 53.7917\n",
      "Iteration 7670 : Loss 53.7916\n",
      "Iteration 7680 : Loss 53.7915\n",
      "Iteration 7690 : Loss 53.7915\n",
      "Iteration 7700 : Loss 53.7914\n",
      "Iteration 7710 : Loss 53.7913\n",
      "Iteration 7720 : Loss 53.7913\n",
      "Iteration 7730 : Loss 53.7912\n",
      "Iteration 7740 : Loss 53.7911\n",
      "Iteration 7750 : Loss 53.7910\n",
      "Iteration 7760 : Loss 53.7910\n",
      "Iteration 7770 : Loss 53.7909\n",
      "Iteration 7780 : Loss 53.7908\n",
      "Iteration 7790 : Loss 53.7908\n",
      "Iteration 7800 : Loss 53.7907\n",
      "Iteration 7810 : Loss 53.7906\n",
      "Iteration 7820 : Loss 53.7906\n",
      "Iteration 7830 : Loss 53.7905\n",
      "Iteration 7840 : Loss 53.7904\n",
      "Iteration 7850 : Loss 53.7904\n",
      "Iteration 7860 : Loss 53.7903\n",
      "Iteration 7870 : Loss 53.7902\n",
      "Iteration 7880 : Loss 53.7902\n",
      "Iteration 7890 : Loss 53.7901\n",
      "Iteration 7900 : Loss 53.7900\n",
      "Iteration 7910 : Loss 53.7899\n",
      "Iteration 7920 : Loss 53.7899\n",
      "Iteration 7930 : Loss 53.7898\n",
      "Iteration 7940 : Loss 53.7897\n",
      "Iteration 7950 : Loss 53.7897\n",
      "Iteration 7960 : Loss 53.7896\n",
      "Iteration 7970 : Loss 53.7895\n",
      "Iteration 7980 : Loss 53.7895\n",
      "Iteration 7990 : Loss 53.7894\n",
      "Iteration 8000 : Loss 53.7893\n",
      "Iteration 8010 : Loss 53.7893\n",
      "Iteration 8020 : Loss 53.7892\n",
      "Iteration 8030 : Loss 53.7891\n",
      "Iteration 8040 : Loss 53.7891\n",
      "Iteration 8050 : Loss 53.7890\n",
      "Iteration 8060 : Loss 53.7889\n",
      "Iteration 8070 : Loss 53.7889\n",
      "Iteration 8080 : Loss 53.7888\n",
      "Iteration 8090 : Loss 53.7887\n",
      "Iteration 8100 : Loss 53.7887\n",
      "Iteration 8110 : Loss 53.7886\n",
      "Iteration 8120 : Loss 53.7885\n",
      "Iteration 8130 : Loss 53.7885\n",
      "Iteration 8140 : Loss 53.7884\n",
      "Iteration 8150 : Loss 53.7883\n",
      "Iteration 8160 : Loss 53.7882\n",
      "Iteration 8170 : Loss 53.7882\n",
      "Iteration 8180 : Loss 53.7881\n",
      "Iteration 8190 : Loss 53.7880\n",
      "Iteration 8200 : Loss 53.7880\n",
      "Iteration 8210 : Loss 53.7879\n",
      "Iteration 8220 : Loss 53.7878\n",
      "Iteration 8230 : Loss 53.7878\n",
      "Iteration 8240 : Loss 53.7877\n",
      "Iteration 8250 : Loss 53.7876\n",
      "Iteration 8260 : Loss 53.7876\n",
      "Iteration 8270 : Loss 53.7875\n",
      "Iteration 8280 : Loss 53.7874\n",
      "Iteration 8290 : Loss 53.7874\n",
      "Iteration 8300 : Loss 53.7873\n",
      "Iteration 8310 : Loss 53.7872\n",
      "Iteration 8320 : Loss 53.7872\n",
      "Iteration 8330 : Loss 53.7871\n",
      "Iteration 8340 : Loss 53.7870\n",
      "Iteration 8350 : Loss 53.7870\n",
      "Iteration 8360 : Loss 53.7869\n",
      "Iteration 8370 : Loss 53.7868\n",
      "Iteration 8380 : Loss 53.7868\n",
      "Iteration 8390 : Loss 53.7867\n",
      "Iteration 8400 : Loss 53.7866\n",
      "Iteration 8410 : Loss 53.7866\n",
      "Iteration 8420 : Loss 53.7865\n",
      "Iteration 8430 : Loss 53.7864\n",
      "Iteration 8440 : Loss 53.7864\n",
      "Iteration 8450 : Loss 53.7863\n",
      "Iteration 8460 : Loss 53.7862\n",
      "Iteration 8470 : Loss 53.7862\n",
      "Iteration 8480 : Loss 53.7861\n",
      "Iteration 8490 : Loss 53.7860\n",
      "Iteration 8500 : Loss 53.7860\n",
      "Iteration 8510 : Loss 53.7859\n",
      "Iteration 8520 : Loss 53.7858\n",
      "Iteration 8530 : Loss 53.7858\n",
      "Iteration 8540 : Loss 53.7857\n",
      "Iteration 8550 : Loss 53.7856\n",
      "Iteration 8560 : Loss 53.7856\n",
      "Iteration 8570 : Loss 53.7855\n",
      "Iteration 8580 : Loss 53.7854\n",
      "Iteration 8590 : Loss 53.7854\n",
      "Iteration 8600 : Loss 53.7853\n",
      "Iteration 8610 : Loss 53.7852\n",
      "Iteration 8620 : Loss 53.7852\n",
      "Iteration 8630 : Loss 53.7851\n",
      "Iteration 8640 : Loss 53.7850\n",
      "Iteration 8650 : Loss 53.7850\n",
      "Iteration 8660 : Loss 53.7849\n",
      "Iteration 8670 : Loss 53.7848\n",
      "Iteration 8680 : Loss 53.7847\n",
      "Iteration 8690 : Loss 53.7847\n",
      "Iteration 8700 : Loss 53.7846\n",
      "Iteration 8710 : Loss 53.7845\n",
      "Iteration 8720 : Loss 53.7845\n",
      "Iteration 8730 : Loss 53.7844\n",
      "Iteration 8740 : Loss 53.7843\n",
      "Iteration 8750 : Loss 53.7843\n",
      "Iteration 8760 : Loss 53.7842\n",
      "Iteration 8770 : Loss 53.7842\n",
      "Iteration 8780 : Loss 53.7841\n",
      "Iteration 8790 : Loss 53.7840\n",
      "Iteration 8800 : Loss 53.7840\n",
      "Iteration 8810 : Loss 53.7839\n",
      "Iteration 8820 : Loss 53.7838\n",
      "Iteration 8830 : Loss 53.7838\n",
      "Iteration 8840 : Loss 53.7837\n",
      "Iteration 8850 : Loss 53.7836\n",
      "Iteration 8860 : Loss 53.7836\n",
      "Iteration 8870 : Loss 53.7835\n",
      "Iteration 8880 : Loss 53.7834\n",
      "Iteration 8890 : Loss 53.7834\n",
      "Iteration 8900 : Loss 53.7833\n",
      "Iteration 8910 : Loss 53.7832\n",
      "Iteration 8920 : Loss 53.7832\n",
      "Iteration 8930 : Loss 53.7831\n",
      "Iteration 8940 : Loss 53.7830\n",
      "Iteration 8950 : Loss 53.7830\n",
      "Iteration 8960 : Loss 53.7829\n",
      "Iteration 8970 : Loss 53.7828\n",
      "Iteration 8980 : Loss 53.7828\n",
      "Iteration 8990 : Loss 53.7827\n",
      "Iteration 9000 : Loss 53.7826\n",
      "Iteration 9010 : Loss 53.7826\n",
      "Iteration 9020 : Loss 53.7825\n",
      "Iteration 9030 : Loss 53.7824\n",
      "Iteration 9040 : Loss 53.7824\n",
      "Iteration 9050 : Loss 53.7823\n",
      "Iteration 9060 : Loss 53.7822\n",
      "Iteration 9070 : Loss 53.7822\n",
      "Iteration 9080 : Loss 53.7821\n",
      "Iteration 9090 : Loss 53.7820\n",
      "Iteration 9100 : Loss 53.7820\n",
      "Iteration 9110 : Loss 53.7819\n",
      "Iteration 9120 : Loss 53.7818\n",
      "Iteration 9130 : Loss 53.7818\n",
      "Iteration 9140 : Loss 53.7817\n",
      "Iteration 9150 : Loss 53.7816\n",
      "Iteration 9160 : Loss 53.7816\n",
      "Iteration 9170 : Loss 53.7815\n",
      "Iteration 9180 : Loss 53.7814\n",
      "Iteration 9190 : Loss 53.7814\n",
      "Iteration 9200 : Loss 53.7813\n",
      "Iteration 9210 : Loss 53.7812\n",
      "Iteration 9220 : Loss 53.7812\n",
      "Iteration 9230 : Loss 53.7811\n",
      "Iteration 9240 : Loss 53.7810\n",
      "Iteration 9250 : Loss 53.7810\n",
      "Iteration 9260 : Loss 53.7809\n",
      "Iteration 9270 : Loss 53.7808\n",
      "Iteration 9280 : Loss 53.7808\n",
      "Iteration 9290 : Loss 53.7807\n",
      "Iteration 9300 : Loss 53.7807\n",
      "Iteration 9310 : Loss 53.7806\n",
      "Iteration 9320 : Loss 53.7805\n",
      "Iteration 9330 : Loss 53.7805\n",
      "Iteration 9340 : Loss 53.7804\n",
      "Iteration 9350 : Loss 53.7803\n",
      "Iteration 9360 : Loss 53.7803\n",
      "Iteration 9370 : Loss 53.7802\n",
      "Iteration 9380 : Loss 53.7801\n",
      "Iteration 9390 : Loss 53.7801\n",
      "Iteration 9400 : Loss 53.7800\n",
      "Iteration 9410 : Loss 53.7799\n",
      "Iteration 9420 : Loss 53.7799\n",
      "Iteration 9430 : Loss 53.7798\n",
      "Iteration 9440 : Loss 53.7797\n",
      "Iteration 9450 : Loss 53.7797\n",
      "Iteration 9460 : Loss 53.7796\n",
      "Iteration 9470 : Loss 53.7795\n",
      "Iteration 9480 : Loss 53.7795\n",
      "Iteration 9490 : Loss 53.7794\n",
      "Iteration 9500 : Loss 53.7793\n",
      "Iteration 9510 : Loss 53.7793\n",
      "Iteration 9520 : Loss 53.7792\n",
      "Iteration 9530 : Loss 53.7791\n",
      "Iteration 9540 : Loss 53.7791\n",
      "Iteration 9550 : Loss 53.7790\n",
      "Iteration 9560 : Loss 53.7790\n",
      "Iteration 9570 : Loss 53.7789\n",
      "Iteration 9580 : Loss 53.7788\n",
      "Iteration 9590 : Loss 53.7788\n",
      "Iteration 9600 : Loss 53.7787\n",
      "Iteration 9610 : Loss 53.7786\n",
      "Iteration 9620 : Loss 53.7786\n",
      "Iteration 9630 : Loss 53.7785\n",
      "Iteration 9640 : Loss 53.7784\n",
      "Iteration 9650 : Loss 53.7784\n",
      "Iteration 9660 : Loss 53.7783\n",
      "Iteration 9670 : Loss 53.7782\n",
      "Iteration 9680 : Loss 53.7782\n",
      "Iteration 9690 : Loss 53.7781\n",
      "Iteration 9700 : Loss 53.7780\n",
      "Iteration 9710 : Loss 53.7780\n",
      "Iteration 9720 : Loss 53.7779\n",
      "Iteration 9730 : Loss 53.7779\n",
      "Iteration 9740 : Loss 53.7778\n",
      "Iteration 9750 : Loss 53.7777\n",
      "Iteration 9760 : Loss 53.7777\n",
      "Iteration 9770 : Loss 53.7776\n",
      "Iteration 9780 : Loss 53.7775\n",
      "Iteration 9790 : Loss 53.7775\n",
      "Iteration 9800 : Loss 53.7774\n",
      "Iteration 9810 : Loss 53.7773\n",
      "Iteration 9820 : Loss 53.7773\n",
      "Iteration 9830 : Loss 53.7772\n",
      "Iteration 9840 : Loss 53.7771\n",
      "Iteration 9850 : Loss 53.7771\n",
      "Iteration 9860 : Loss 53.7770\n",
      "Iteration 9870 : Loss 53.7769\n",
      "Iteration 9880 : Loss 53.7769\n",
      "Iteration 9890 : Loss 53.7768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9900 : Loss 53.7768\n",
      "Iteration 9910 : Loss 53.7767\n",
      "Iteration 9920 : Loss 53.7766\n",
      "Iteration 9930 : Loss 53.7766\n",
      "Iteration 9940 : Loss 53.7765\n",
      "Iteration 9950 : Loss 53.7764\n",
      "Iteration 9960 : Loss 53.7764\n",
      "Iteration 9970 : Loss 53.7763\n",
      "Iteration 9980 : Loss 53.7762\n",
      "Iteration 9990 : Loss 53.7762\n",
      "Iteration 10000 : Loss 53.7761\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for i in range(1, 10001):\n",
    "    \n",
    "    dW, db = gradient_diabetes(X_train, w, b, y_train)\n",
    "    w -= LEARNING_RATE * dW\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss_function(X_train, w, b, y_train)\n",
    "    losses.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5c51b84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD4CAYAAADmWv3KAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWG0lEQVR4nO3df5Bd5X3f8ff33rvaRT9AkrWAQNQSDuARHsemCgGnTYkJhmQ8xp2pO6JDK7dkmKSetHWmSWHcKdM/6Dhpxm0yqZMyMTZ1XIhCnJhxJ44paerWSSBLKET8EMgDRmsEWpkAkjAr7e63f9yzq7t3z2pXu1e6e47er5mde+9zzr33+2jhs899znPOjcxEklQ/jX4XIEk6PQx4SaopA16SasqAl6SaMuAlqaZa/S4AYNOmTbl169Z+lyFJlfL4448fyszh+baviIDfunUrIyMj/S5DkiolIr57su1O0UhSTRnwklRTBrwk1ZQBL0k1ZcBLUk0Z8JJUUwa8JNVUpQP+1Tff4XPf3Mt3xo70uxRJWnEqHfAHD7/Dr//JPl46dLTfpUjSilPpgG9EADDld5ZI0hyVDvgi35nyW6kkaY5KB/z0CN6vHZSkuWoR8E7RSNJcFQ/49q1TNJI0V6UDPhzBS9K8Kh3w0yN45+Alaa6KB/z0CN6Al6Ru9Qj4qT4XIkkrUKUD3nXwkjS/Sgd8ozG9Dr7PhUjSClTtgHcEL0nzWjDgI+LeiDgYEXu62n8+IvZGxNMR8Ssd7XdGxL5i242no+hpnugkSfNrLWKfLwG/Afy36YaI+AngZuD9mTkeEecX7duBncCVwEXA/4yIyzNzsteFt9+vfesIXpLmWnAEn5nfAl7vav454LOZOV7sc7Bovxl4IDPHM/NFYB9wdQ/rncVr0UjS/JY6B3858Hcj4tGI+N8R8SNF+8XA/o79Rou2OSLi9ogYiYiRsbGxJRXhFI0kzW+pAd8CNgDXAL8I7I72dQOiZN/S+M3MezJzR2buGB4eXlIRHmSVpPktNeBHga9m22PAFLCpaL+kY78twCvLK3F+XotGkua31ID/Q+DDABFxObAKOAQ8BOyMiMGI2AZcBjzWgzpLeS0aSZrfgqtoIuJ+4DpgU0SMAncB9wL3FksnjwG7sp2yT0fEbuAZYAL41OlaQQNei0aSTmbBgM/MW+bZdOs8+98N3L2cohbLg6ySNL9Kn8nqOnhJml+lA/7EOvg+FyJJK1DFA759O+UcjSTNUfGAdw5ekuZT6YB3Dl6S5lfxgA8iXAcvSWUqHfDQnqZxikaS5qpBwDtFI0llKh/w4QhekkpVPuAbzsFLUqkaBHw4RSNJJWoS8P2uQpJWnsoHfHiQVZJKVT7gGxFei0aSStQg4B3BS1KZGgS8B1klqUzlA9518JJUrvIB7zp4SSpXg4APpqb6XYUkrTw1CHgPskpSmcoHvHPwklSu8gHfaDgHL0llqh/wLpOUpFI1Cfh+VyFJK0/lA95r0UhSuQUDPiLujYiDEbGnZNu/joiMiE0dbXdGxL6I2BsRN/a64G5ei0aSyi1mBP8l4Kbuxoi4BLgBeLmjbTuwE7iyeM7nI6LZk0rn4TJJSSq3YMBn5reA10s2/Sfgl4DOdL0ZeCAzxzPzRWAfcHUvCp2PB1klqdyS5uAj4mPA9zLzya5NFwP7Ox6PFm1lr3F7RIxExMjY2NhSyph+HQ+ySlKJUw74iFgNfAb4d2WbS9pK4zcz78nMHZm5Y3h4+FTLmOG1aCSpXGsJz3kPsA14MiIAtgB/FRFX0x6xX9Kx7xbgleUWeTIuk5Skcqc8gs/Mv87M8zNza2ZupR3qV2Xmq8BDwM6IGIyIbcBlwGM9rbiLB1klqdxilkneD/w5cEVEjEbEbfPtm5lPA7uBZ4BvAJ/KzMleFTtPfY7gJanEglM0mXnLAtu3dj2+G7h7eWUtnnPwklSu8meyukxSksrVI+D9wg9JmqPyAe+1aCSpXOUD3mvRSFK56gd8wxG8JJWpfsB7kFWSSlU+4F0HL0nlKh/wroOXpHI1CHhH8JJUpgYB70FWSSpT+YB3Dl6SylU+4J2Dl6RyNQh4l0lKUpmaBHy/q5CklafyAe+1aCSpXOUD3mvRSFK5GgS8I3hJKlODgA8mnYSXpDmqH/CNYMqAl6Q5Kh/wzQgmnaKRpDkqH/CNRjDpV/ZJ0hyVD/hWwxOdJKlM5QO+2QgmHMJL0hyVD3jPZJWkcpUP+GYDl0lKUokFAz4i7o2IgxGxp6PtP0bEcxHxVET8QUSs79h2Z0Tsi4i9EXHjaap7RqPhKhpJKrOYEfyXgJu62h4G3peZ7weeB+4EiIjtwE7gyuI5n4+IZs+qLdFyHbwklVow4DPzW8DrXW3fzMyJ4uFfAFuK+zcDD2TmeGa+COwDru5hvXM0I5gw4CVpjl7Mwf8z4I+K+xcD+zu2jRZtc0TE7RExEhEjY2NjS37zRiMAHMVLUpdlBXxEfAaYAL4y3VSyW2nyZuY9mbkjM3cMDw8vuYZmtN/SeXhJmq211CdGxC7go8D1eeI780aBSzp22wK8svTyFjY9gp+cSgZO62y/JFXLkkbwEXET8G+Aj2Xm2x2bHgJ2RsRgRGwDLgMeW36Z82tNT9E4gpekWRYcwUfE/cB1wKaIGAXuor1qZhB4ONpTJH+RmT+bmU9HxG7gGdpTN5/KzMnTVTy0z2QFPNAqSV0WDPjMvKWk+Qsn2f9u4O7lFHUqGuFBVkkqU4MzWU/MwUuSTqh8wM8cZHUOXpJmqXzAzxxk9YKSkjRL5QN+eh38hAkvSbNUPuAbjuAlqVTlA75Z9MA5eEmarfIBP71M0lU0kjRb5QO+1Wh3wTNZJWm2ygf89BTNxKQBL0mdKh/wM2eyOoKXpFkqH/CeySpJ5Sof8J7JKknlKh/wLb/RSZJKVT7gT5zJasBLUqfKB7zfySpJ5Sof8E3n4CWpVOUD3jNZJalc5QPe72SVpHKVD/iZ72T1TFZJmqXyAe+ZrJJUrvIBf+JM1j4XIkkrTA0Cvn3rKhpJmq0GAV9cLthVNJI0S/UD3jNZJalU5QO+GMA7gpekLpUPeM9klaRyCwZ8RNwbEQcjYk9H28aIeDgiXihuN3RsuzMi9kXE3oi48XQVPq3pmaySVGoxI/gvATd1td0BPJKZlwGPFI+JiO3ATuDK4jmfj4hmz6ot4Rd+SFK5BQM+M78FvN7VfDNwX3H/PuDjHe0PZOZ4Zr4I7AOu7k2p5VrFOsnjLoSXpFmWOgd/QWYeAChuzy/aLwb2d+w3WrTNERG3R8RIRIyMjY0tsQwYaLqKRpLK9Poga5S0lSZvZt6TmTsyc8fw8PCS33BgegQ/4QhekjotNeBfi4jNAMXtwaJ9FLikY78twCtLL29h01eTPO4IXpJmWWrAPwTsKu7vAr7W0b4zIgYjYhtwGfDY8ko8uYig1QgmnIOXpFlaC+0QEfcD1wGbImIUuAv4LLA7Im4DXgY+AZCZT0fEbuAZYAL4VGZOnqbaZ7Sa4Ry8JHVZMOAz85Z5Nl0/z/53A3cvp6hTNdBsuIpGkrpU/kxWMOAlqUwtAr49B+8UjSR1qkXAt0fwBrwkdapFwLcPsjpFI0mdahHwzsFL0ly1CPhWI5yikaQutQj4gWbDE50kqUstAt4TnSRprloE/ECzwTEvNiZJs9Qk4B3BS1K3WgR8q+EcvCR1q0XADzRdRSNJ3WoS8K6Dl6RutQj4VrPhHLwkdalFwA80whG8JHWpRcC3ml5NUpK61SLgnYOXpLkMeEmqqVoEfKvhiU6S1K0WAT/Q8lIFktStFgE/1GoyMZVMOoqXpBm1CPjBgXY3xicm+1yJJK0ctQj4oVa7G+8cd5pGkqbVIuAHB5qAI3hJ6lSLgB8acAQvSd2WFfAR8emIeDoi9kTE/RExFBEbI+LhiHihuN3Qq2LnM9Rqj+DfOe4IXpKmLTngI+Ji4F8AOzLzfUAT2AncATySmZcBjxSPT6sTB1kdwUvStOVO0bSAcyKiBawGXgFuBu4rtt8HfHyZ77EgR/CSNNeSAz4zvwf8KvAycAB4MzO/CVyQmQeKfQ4A55c9PyJuj4iRiBgZGxtbahmAI3hJKrOcKZoNtEfr24CLgDURcetin5+Z92TmjszcMTw8vNQyABh0BC9JcyxniuYngRczcywzjwNfBT4EvBYRmwGK24PLL/PkTqyiMeAladpyAv5l4JqIWB0RAVwPPAs8BOwq9tkFfG15JS5segTvFI0kndBa6hMz89GIeBD4K2ACeAK4B1gL7I6I22j/EfhELwo9maHpE50cwUvSjCUHPEBm3gXc1dU8Tns0f8Z4kFWS5qrHmazFFM0PjjmCl6RptQj4Va0Gq5oNjhrwkjSjFgEPsHaoxZHx4/0uQ5JWjNoE/LqhFoffmeh3GZK0YtQm4NcOtjhiwEvSjFoF/OFxA16SptUm4NcNOYKXpE61Cfi1gy2OOIKXpBn1CfghA16SOtUn4AcHOPzOcTKz36VI0opQm4Bfv3qA45PpyU6SVKhNwG9aOwjAocPjfa5EklaGGgX8KgAOHTHgJQlqFfDFCN6AlySgRgE/vK4d8GNHjvW5EklaGWoT8BvXFFM0zsFLElCjgB9oNhheN8grb/yg36VI0opQm4AH2PauNbz0/aP9LkOSVoRaBfzWTat56ftv97sMSVoRahXw737XGsYOj3vJAkmiZgF/+QXrAHjuwFt9rkSS+q9WAf+BS9YD8MTLb/S1DklaCWoV8MPrBrlk4zk8/t2/6XcpktR3tQp4gGsvfRff3neIYxNT/S5FkvqqdgF/0/su5PD4BH/2nUP9LkWS+mpZAR8R6yPiwYh4LiKejYhrI2JjRDwcES8Utxt6Vexi/NgPbeLcoRa7R/afybeVpBVnuSP4XwO+kZnvBX4YeBa4A3gkMy8DHikenzGDrSb/6EffzTf2vMr+110TL+nsteSAj4hzgR8HvgCQmccy8w3gZuC+Yrf7gI8vr8RT98kPbWWg2eCXv/HcmX5rSVoxljOCvxQYA74YEU9ExG9HxBrggsw8AFDcnl/25Ii4PSJGImJkbGxsGWXMdeF5Q/zs33sPX3/qAH+692BPX1uSqmI5Ad8CrgJ+MzM/CBzlFKZjMvOezNyRmTuGh4eXUUa5n7vuPVxxwTp+YfeTXoBM0llpOQE/Coxm5qPF4wdpB/5rEbEZoLjtyxB6aKDJ52+9imMTU/zjLzzKmJcRlnSWWXLAZ+arwP6IuKJouh54BngI2FW07QK+tqwKl+E9w2u595M/witvvMMnfuvPeP61w/0qRZLOuOWuovl54CsR8RTwAeA/AJ8FboiIF4Abisd9c/W2jfzOz/woR8Yn+fv/5dt8+c9fYmoq+1mSJJ0Rkdn/sNuxY0eOjIyc1vc48OYP+MXfe4r/u+8Q799yHp++4XKuu3yYiDit7ytJp0tEPJ6ZO+bbXrszWeez+bxz+PJtV/O5f/jDfP/IMf7pF/+Sn/q1/8MXv/0irx/1e1wl1c9ZM4LvdGxiij94YpSvPPoyT42+SSPgqr+1gZ947/lcc+lGrrzoPIYGmmesHklaioVG8GdlwHd69sBb/I+nDvCnzx9kz/fa15FvNYIrLlzHey88l22bVrNt01q2blrNhecOsWH1KhoNp3Uk9Z8BfwrGDo/zxMt/w5Ojb/DU6Js8/9phXntr9vLKViPYtHaQ4XWDbFyzinVDLdYNtVg72GLd0ABrB1usGWyyqtVgVbPJYKvRvj/902wwNNCg2WjQjCACmo2gEUGjAc2Yvh9FOzQiTuwTeNxAErBwwLfOZDEr3fC6QT5y5YV85MoLZ9rePjbBS4fe5qXvH+XgW+8wdmScg2+NM3ZknNePHmP/629zeHyCI+9M8IPjk2e85umsj5nHMfP4xLbZO0XHc6e3lb3OzJ+R+d5jgb8zi/kztJg/Vot7nUXstMArLeY1elVLLOKVelFPrwYDi6qlB/3u3e+gN/9dLWan5f4Orrt8mH/70e2LqeaUGfALWL2qxfaLzmX7RecuuO/xySmOjk9w9NgkxyamODYxxfhEx/3J6bYpJqemmJqCyUymppKpbN/PTCan2j9ZtLXvJ5NTMJXJzGeu4tNXzn5Ikh33526b3nBiW3btM3f/7g96C33yW8znwsV8eMxFvNLiXmf5r7GYXi2qljPU7zP5O+jFLouZTehdnxbzOj2oZxFvtHn9OYuoZmkM+B4aaDZYv3oV61f3uxJJOouWSUrS2caAl6SaMuAlqaYMeEmqKQNekmrKgJekmjLgJammDHhJqqkVcS2aiBgDvruMl9gEHOpROVVwtvUX7PPZwj6fmndn5rxfar0iAn65ImLkZBfcqZuzrb9gn88W9rm3nKKRpJoy4CWppuoS8Pf0u4Az7GzrL9jns4V97qFazMFLkuaqywhektTFgJekmqp0wEfETRGxNyL2RcQd/a5nqSLikoj4XxHxbEQ8HRH/smjfGBEPR8QLxe2GjufcWfR7b0Tc2NH+tyPir4ttvx4r/AtcI6IZEU9ExNeLx7Xuc0Ssj4gHI+K54vd97VnQ508X/13viYj7I2Kobn2OiHsj4mBE7Olo61kfI2IwIn63aH80IrYuqrAsviauaj9AE/gOcCmwCngS2N7vupbYl83AVcX9dcDzwHbgV4A7ivY7gF8u7m8v+jsIbCv+HZrFtseAa2l/VeQfAT/V7/4t0PdfAP478PXica37DNwH/ExxfxWwvs59Bi4GXgTOKR7vBj5Ztz4DPw5cBezpaOtZH4F/DvxWcX8n8LuLqqvf/zDL+Ae9Fvjjjsd3Anf2u64e9e1rwA3AXmBz0bYZ2FvWV+CPi3+PzcBzHe23AP+13/05ST+3AI8AH+ZEwNe2z8C5RdhFV3ud+3wxsB/YSPsrQr8OfKSOfQa2dgV8z/o4vU9xv0X7zNdYqKYqT9FM/4czbbRoq7Tio9cHgUeBCzLzAEBxe36x23x9v7i4392+Uv1n4JeAqY62Ovf5UmAM+GIxLfXbEbGGGvc5M78H/CrwMnAAeDMzv0mN+9yhl32ceU5mTgBvAu9aqIAqB3zZ/Ful13xGxFrg94F/lZlvnWzXkrY8SfuKExEfBQ5m5uOLfUpJW6X6THvkdRXwm5n5QeAo7Y/u86l8n4t555tpT0VcBKyJiFtP9pSStkr1eRGW0scl9b/KAT8KXNLxeAvwSp9qWbaIGKAd7l/JzK8Wza9FxOZi+2bgYNE+X99Hi/vd7SvRjwEfi4iXgAeAD0fE71DvPo8Co5n5aPH4QdqBX+c+/yTwYmaOZeZx4KvAh6h3n6f1so8zz4mIFnAe8PpCBVQ54P8SuCwitkXEKtoHHh7qc01LUhwp/wLwbGZ+rmPTQ8Cu4v4u2nPz0+07iyPr24DLgMeKj4GHI+Ka4jX/ScdzVpTMvDMzt2TmVtq/uz/JzFupd59fBfZHxBVF0/XAM9S4z7SnZq6JiNVFrdcDz1LvPk/rZR87X+sf0P7/ZeFPMP0+MLHMgxo/TXvFyXeAz/S7nmX04+/Q/rj1FPD/ip+fpj3H9gjwQnG7seM5nyn6vZeO1QTADmBPse03WMSBmH7/ANdx4iBrrfsMfAAYKX7XfwhsOAv6/O+B54p6v0x79Uit+gzcT/sYw3Hao+3betlHYAj4PWAf7ZU2ly6mLi9VIEk1VeUpGknSSRjwklRTBrwk1ZQBL0k1ZcBLUk0Z8JJUUwa8JNXU/wd+oHHo6c1fYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8953c8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.71758393594395"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = Model_diabetes(X_test, w, b)\n",
    "mse = loss_function(X_test, w, b, y_test)\n",
    "mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "52a1effd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqqklEQVR4nO2de5RcVZ3vP7/udCcd1IQ8IOlOYpgx4oKABBqWy2TNKEyAEZEoGrhcFWdkoleUwUegc3WFGOWmSUbQeAfHCC6Z6wNakBBAJsYE9SY+oMMjPHN5Kt0dSAIkCOmkX/v+UadJV/U53afqvPY+9fus1au6dp1Ttc+pU7+z9/f32GKMQVEURckXNVl3QFEURYkfNe6Koig5RI27oihKDlHjriiKkkPUuCuKouSQMVl3AGDKlClm9uzZWXdDURTFKbZv377XGDPV7zUrjPvs2bNpb2/PuhuKoihOISJ/DnpNZRlFUZQcosZdURQlh6hxVxRFySFq3BVFUXKIGndFUZQcYkW0TLWz/sFO1mzcSde+bhonNrD0rGNZNK8p624piuIwatwzZv2DnSz7xSN09/YD0Lmvm2W/eARADbyiKBWjskzGrNm4803DPkh3bz9rNu7MqEeKouQBNe4Z07Wvu6x2RVGUMKhxz5jGiQ1ltSuKooRBjXvGLD3rWBrqaovaGupqWXrWsRn1SFGUPKAO1YwZdJpqtIyiKHEyqnEXkXHA74Cx3va3GmOuEpFJwC3AbOB5YLEx5lVvn2XAp4F+4DJjzMZEep8TFs1rUmOuKEqshJFlDgGnG2PeDZwEnC0i7wFagM3GmDnAZu85InIccCFwPHA2cL2I1Pq9saIoipIMoxp3U+B172md92eA84CbvPabgEXe/+cBNxtjDhljngOeBk6Ls9OKoijKyITS3L2R93bgHcC/G2P+JCJHG2N2ARhjdonIUd7mTcAfh+ze4bWVvucSYAnArFmzKj8CRXEYzU5WkiKUcTfG9AMnichE4HYRmTvC5uL3Fj7vuQ5YB9Dc3DzsdUXJO1lnJ+uNpUx2tMHmlbC/AybMgDOWw4mLs+5VIGWFQhpj9gG/oaClvyQi0wG8x93eZh3AzCG7zQC6onZUUfJGltnJgzeWzn3dGA7fWNY/2Jn4ZzvJjja48zLY/wJgCo93XlZot5RRjbuITPVG7IhIA/APwJPABuBib7OLgTu8/zcAF4rIWBE5BpgD3BdzvxXFebLMTtayF2WyeSX0lnwvvd2FdksJI8tMB27ydPcaoM0Yc5eI/AFoE5FPA38BPgZgjHlMRNqAx4E+4FJP1lEUZQiNExvo9DHkaWQna9mLMtnfUV67BYxq3I0xO4B5Pu0vA2cE7HM1cHXk3ilKjll61rFFmjukl52c5Y3FSSbM8CQZn3ZL0fIDipIRi+Y1seojJ9A0sQEBmiY2sOojJ6Ti1NSyF2VyxnKoK7nx1TUU2i1Fyw8oSoZklZ2sZS/KZDAqxqFoGTEm+yjE5uZm097ennU3FEVRnEJEthtjmv1eU1lGURQlh6hxVxRFySGquSuKkhiaBZsdatwVRUmErMsrVDsqyyiKkgiaBZstatwVRUkEzYLNFjXuiqIkgi7+ni1q3BVFGZH1D3Yyv3ULx7TczfzWLaErR2oWbLaoQ1VRlECiOEU1CzZb1Lgr1qBhc/YxklM0zHeji79nhxp3xQo0bM5O1CnqLqq5K1agYXN2ok5Rd1HjrliBjhDtRJ2i7qLGXbECHSHaSZY155VoqOauWEGWqxIpI6NOUTdR465YgYbNlY+r0UWu9ts11Lgr1qAjxPC4Gl3kar9dRDV3RXEQV6OLXO23i6hxVxQHcTW6yNV+u4jKMo6j+uUo7GhzalHjsDRObKDTxyDaHl3kar9dREfuDjOoX3bu68ZwWL8MW9gp9+xogzsvg/0vAKbweOdlhXbHcTX+3NV+u4gad4dR/XIUNq+E3pJRYm93od1xXI0/d7XfLqKyjMOofjkK+zvKa3cMV6OLXO23a6hxdxjVL0dhwgxPkvFprxD1cSiuoLKMw6h+OQpnLIe6khtdXUOhvQLUx6G4xKjGXURmisi9IvKEiDwmIv/qta8QkU4Recj7+8CQfZaJyNMislNEzkryAKoZ1S9H4cTFcO5amDATkMLjuWsrjpZRH4fiEmFkmT7gy8aYB0TkrcB2EdnkvXadMebfhm4sIscBFwLHA43Ar0XkncaY4l+FEguqX47CiYtjC31UH0f2qCwWnlFH7saYXcaYB7z//wo8AYx0Ns8DbjbGHDLGPAc8DZwWR2cVJUu0cmVM7GiD6+bCiomFx5ChqSqLlUdZmruIzAbmAX/ymj4vIjtE5IcicqTX1gQM9WJ14HMzEJElItIuIu179uwpv+eKkjLq44iBCLkHKouVR2jjLiJvAW4DLjfGvAZ8D/hb4CRgF/CtwU19djfDGoxZZ4xpNsY0T506tdx+K0rqqI8jBiLkHqgsVh6hQiFFpI6CYf+JMeYXAMaYl4a8/gPgLu9pBzBzyO4zgK5YeqsoZRK3Rqs+johEyD3Q0N/yCBMtI8CNwBPGmGuHtE8fstmHgUe9/zcAF4rIWBE5BpgD3BdflxUlHKrRWkhQjkGI3AOVxcojjCwzH/gEcHpJ2ONqEXlERHYA7we+CGCMeQxoAx4H/gu4VCNllCxQjdZCIuQeqCxWHqPKMsaYrfjr6L8cYZ+rgasj9EtRIqMarYUMhqVWWKlTZbHwaPkBJbeoRls+qcSRx5h7oASj5QeU3KIabXkk4qOoMKZdiY4adyW3qEZbHrH7KHJcT98FVJZJEE2Vzp6sNFoXv/vYfRQjxbSrLJM4atwTQld5r15c/e5j91HkvJ6+7agskxAahle9uPrdx+2jONAwrax221n/YCfzW7dwTMvdzG/dYn2+hBr3hNAwvOrF1e8+bh/F6t4LOGDqi9oOmHpW914QQ2/TxcWEOJVlEkLD8KoXl7/7OH0UN71+Gq/U9HDFmDYa5WW6zGRW9y3mzkOnsSKWT0iPkWZjtkptOnJPCA3Dq170uy/QOLGBDQMLWNCzlr859BMW9Kxlw8ACJ25ypbg4G1PjnhAahle96HdfIE83ORdr+Ysxw6rxpk5zc7Npb2/PuhuKosSMiyGhfpRGQEHhRpX1TVtEthtjmv1eU81dUciPESqLHW0V13gJS15qwQweg0vXiBp3ZRjVZuhcjUuPxGD26GCS0WD2KGiCUQCu3ajUuCtFVKOhczESIjIB2aMH7lnOwl9OqZobe55Rh6pShKsJOFFwMRIiMgFZouMOvOhULLcSjBp3pYhqNHQuRkJEJmDloy4zueh53m/seUaNu1JENRq6PIXshcZnRaQDpp7VfcP19ubXNmnZXgdR464UEWTo3v+uqU7V1SiHqoxLP3ExnLsWJswEBCbMZHXd59gwsKBosw/VbKW1/kYt2+sgGueuDKM0Wub975rKbds7rYvxVeLFL5Z729jLaJK9wzeeMBO++GiKvVP80Dh3pSxKQ77mt26pvmiSKsQvlrvx4Mv+G2vZXutR466MSjU6WauVYbHc183wJJkSAhyyij2o5m4Dlq8zWY1O1lwR5frycbxS11BoV6xGjXvW2LjOZIkx+PZxT1VfNEleiHp9+TheOXetZrE6gDpUs+a6uQHT3owcVqVp6QB1Ddx/wte5/PE5mrkYN0nXd7Ht+lJiRR2qNmPbOpMBaemnPvNdtrWoMYiVNOq72HZ9RSWFYmd5QY171kzwd1gdaJjGwtYt6Y+UA43BC94oUH9UsRFwI2XzyvjObcD1laVDtOLCdFrsrCxUc88aH4dVX+04lr9xfjY1PgJ/9GKXXyAPpDGqtswhGmkt0pFuhsowRjXuIjJTRO4VkSdE5DER+VevfZKIbBKRp7zHI4fss0xEnhaRnSJyVpIHUCnWrGTu47D6pnyWW3veW7RZajU+/IwBApT4ZvRHNTJhIlSCbqRxjqotc4hGKkyXN4kpYcLIMn3Al40xD4jIW4HtIrIJ+BSw2RjTKiItQAtwpYgcB1wIHA80Ar8WkXcaY/oD3j91rCtre+Lioh/bTS13+26WSlz5YD+G6pp+03rQH1UQYeWDM5b7Oq9jH1WXXF9ZEilnwkKJyWZGHbkbY3YZYx7w/v8r8ATQBJwH3ORtdhOwyPv/POBmY8whY8xzwNPAaTH3OxK2l7XNPK78xMWFSIoV+wqPE2b6bxf2R2V5HH/shJUPLBtVp0Gka9syicl2ytLcRWQ2MA/4E3C0MWYXFG4AwFHeZk3A0Ntrh9dW+l5LRKRdRNr37NlTQdcrx/aMS+uqFEb5UdkYx5805cgHpTfSHBt2iHhtV+HNMAqho2VE5C3AbcDlxpjXRCRwU5+2YcH0xph1wDooxLmH7UccNE5soNPHkNuScWndeo1+Uk3YaJk0IkLiIM4QO5UPAol8bVskMdlOKOMuInUUDPtPjDG/8JpfEpHpxphdIjId2O21dwBD5/EzgK64OhwHS8861nclc5syLq1br7HSH5ULoZVxh9ilpaU7inXXdk4JEy0jwI3AE8aYa4e8tAG42Pv/YuCOIe0XishYETkGmAPcF1+Xo5Na/e5q05r9cCG0Mu4QO5UPFAsYtfyAiCwA/i/wCDDgNf9PCrp7GzAL+AvwMWPMK94+XwX+mUKkzeXGmHtG+oxclh8ISONP60decaJI3PidB7/QSsguJX7FRHz7gxS0cFvRbM2qJ1L5AWPMVvx1dIAzAva5Grg6dA/zSIZas1Whni6EVrqokbuSrak3oMzQ8gNJkVLChd8IfaRQTxvi+AOLWTUcmY0O76JG7oKj2pUbUE7R8gNJkUL2YVAqt18kENgT6skZy+mrHVfU1C91cOiv2ejwLmrkLmRrarmATNGRe1IkMBosHaW/cajPd4ReK0K/jy/FllDP9f3z2dp7CZdzM43yMl1mMuPlIJPk9eIN0xyJuhZi54KU5MINKMeocU+KKLHhPvjp6EH0G0NDXa21oZ5rNu6ks+e93Mrh+jnPjr3If+Mgfb7acUFKcuEGlGPUuCdJjKNBPx09iKYh2nvm0TI++MlDA9RQ82Yw1hCkdnibEvvgIRFcuAEFkQNHsBr3tKnwogmrlw+O0P0SRWwJj/TLEK71M+wA9tSby5a7vgTbf1Q4H1ILp3wKPnit3QbHhRuQHwGO4Puff9Wp1cjUuKdJhOiBoJIJR46vY3z9mFEvOJvCI/0yhLuYQhN7h28cVLSsmrjrS9B+4+Hnpv/w8w9e67+PLbjmy4BAR3Dj9tV0HloLWFBJNgQaLZMmEaIHggouXXXu8WxrOZ3nWs9hW8vpgReaTZUw/TKEu065Qiv+BbH9R+W1K9EIcPhO5+Wi5zZVkvVDR+5pEiF6IGrBJdsqYQ6XjU6H2Ue6N4VPgyBpKqxklQP9OFUCHMFdZvLwNlvCi31Q454mEaMHohRcsr0SJuDmFD4NpNbfkIdxNo8kBYIafT98HMHdjGV13/BzY9XvpwSVZdIkw8UGrKsRr4TnlE+V1z6UICnwniurr85+WHyS2h49+RvUj6lha/1lPDv2IrbWX8ZH639v9e9n1MJhaZDLwmFBZDhFtiVaRqmAoGiZ0QgsihZARsXbrL82d7TRd8cXGNN/8M2mvtpxjDnvu5nOdiIVDlNiRqUHpRI+eG1lkTEjFWrzI4PsUZsiuQLZvLLIsAOF5zbV8ilBZZkqYf2DnWy9/XpuOfAvPDP2Im458C9svf161j/Ymcpnz2/dwjEtdzO/dUsqn6l4BEmBDZP8t88ge9SmSK5AHCyloCP3KuGhu9exUtYxXnoAmCF7WWnWsfruMSya9/XEPjdoVNb+51e498k99k7D80JQIhFYkz1qWySXLwmUUkhailLjXiVc0vNjxtf0FLWNlx4u6fkxkJxxDxqV/eSPf3lTCbZyGp4nRpICLYiWcSKSK+ZSCmlIUSrLVAmNNT7ZnyO0x0XQ6KvUxWfdNLwaOHFxwXm6Yl/hMSPt2IlIrpjLQqchRenI3SWiRNpIDRi/wlzJ3t+DRmV+WDUNtwzro0kiEDVBLzViDIZIQ4pS4+4KEVe1qfEz7CO0x4VfHZmAFVTtmoYHkUEoa2JT+ErDKyH28xAlQc9F0pCiVJZxhZGSUa6bW4hnvm5ucBJKUAGuhAtz+dWR+e/vmWX/NNyPwRtsyok/iUzhB4uRDWa+DhYju+tLo++b0XnIE2lIUTpyd4WgkKvuVwp/MPJoPsPa2n6jsvNqfs/MB9ZwlNnDbpnKCycv5dR5Zyfel0hktG5pIlP4kYqRjTZ6d2H9VstJQ4qqWuPunIYZNhkl6EdmU23tHW2c+shVQDcITGMP0x65qlA4zGbjkFGscyJT+CjFyByM+baRpKWoqpRlghaWtjq5xi8ZJYigH5kl0RHOLpycwqLnfiQyhQ8sOiajy3wZnQelPKrSuDuREVeKXyhW3RH+2zYcmWrXysbVkd+cM8trjwk/v8Wqj5wQbdR3yqeGObUNQE3N6Fp6hgXwlPBUpSzjREacH6WhWNccA71vZNefSnF14eSnflVee4zEPYVf3/Rl3rjvL1wgm6llgH5qOEg9bxkorp/iK/PZJPEpgVSlcXciIy4M3a+W124LjiycXOqX2XqwA/Hb0PYZhw9rNu6ks+ef+Cr/9Gbbs2Mv8t/Y7/i0AJ71VKUs40RGXBhc1T5jzvZLAj+/jN9KPID159uvcJvfLLXLTPF/A8uPT/FnVOMuIj8Ukd0i8uiQthUi0ikiD3l/Hxjy2jIReVpEdorIWUl1PAqJaJhZ4LL2aYtzNwA/v8w1vYvpZmzxhnUNBc09TK5BBgQFD0xoqBu27eq+gONz4XpShhFGlvkR8L+B/yxpv84Y829DG0TkOOBC4HigEfi1iLzTmLCLPaZHLjLiVPtMDL+R7YaBBUgPfGfqnYfP95wz4eGfVpw5nDRBwQPj6mpoqKstem1T7d/ziZNnc+oz39XrKQ0SznYe1bgbY34nIrNDvt95wM3GmEPAcyLyNHAa8IfKu6iMiGqfiRDkl2l/20L44qrDDdfNtTqhJyhIYN+BXq674KRhuR6FRLLPpNvJaiRiOZEwRHGofl5EPgm0A182xrwKNAF/HLJNh9c2DBFZAiwBmDVrVoRuKEr8+NXE8fXLWB7WOVLwQC5mr66SQpZvpQ7V7wF/C5wE7AK+5bX7BRP4LuBojFlnjGk2xjRPnTq1wm4oSjKE9stY7tR2JnhgR5u1fotESGFQUNHI3Rjz0uD/IvID4C7vaQcwtBLVDKCr4t4po5Phgtt5J9TI1vKwTifK6aYgUVhHCrkeFRl3EZlujNnlPf0wMBhJswH4qYhcS8GhOge4L3IvFX92tMEdl0K/t8LS/hcKzyG/PwrbcMCpbb38Uo2FyFIYFIxq3EXkZ8D7gCki0gFcBbxPRE6iILk8j+eBMcY8JiJtwONAH3BpkpEyzhX/KocwI/J7rjxs2Afp7ym05/VHYSMpOLVTu9bTmAmWfkZQQTxL/BaJkMKgQIzxlcRTpbm52bS3t5e1T+kCBlDQEp2MVy+ldJoKhbt6aaLPignB77Fif3L9U1IltWs97HUX92cELd8yYWYhB0IJRES2G2Oa/V5zNkPVyeJfYYmjamK1OahyTGrXehrVOv0+A8OwWAyL/Bau4mxtGWeLf4UhrCe9YdLhhTqGUndE9TmoUiILKTC1az2NsM7A9zKFkbqlfgsXcda456b4lx9hPen/eA2s/xwM9B5uq6mDMWOHG/2cO6jSMLqJrWU6Cqld62lU6wz8DJVg4sZZWcaZ+N1KCFsz5sTFsOj64gJci64PrgqZUwdVWouvZCUFpnatp1GryOV6SI7h7MjdifjdSinHk+4XqbF5ZXb10jOIux/J6MZ5PWQlBaZ2racR1ulA6GhecDZaRhmBNKIeLPrcY1ru9k2DFuC51nPCvUmIm9L81i2+8kjTxAa2tZxefscVJSK5jJZRCvjV6s6sXnpGa6MGac+hNenBm9Ioy8vlWgpUcoezsowymoMv3sSaUA7LjIpohS7yFUTIDMlcS4FK7lDj7jBpac2ho0QyWhs1stEt46ZkfSq/oniocXeYtBx8azbuZGH/b7mivo1G2UuXmcLqvsWs2VhfbOgyLKIVyei6umC3ooyAau4OE1lrDknza5torbuBGTV7qRGYUbOX1robaH5tU/GGDqyN6ouG5yk5REfuDhNZaw7JsvqfM57iAmXjpYdl9T8HVhVvHKGIViKJSGFCMzU8T8khatwdJi0H39HsLau9EhLJ/iynTnjclR21zr6SMWrcHScNB58EaNISoyadiHM4qzrh1bj4hGIdbmvuWvkwHZLQpEu+u2H6vUck53BW65tmFO9fLr45EkpucHfkrqOj9Ihbk/b57lrrb8T0wIaBBUWbRnIOlxMFE6eMYvmi2ZBdETQlPdwduTsyOsoNJy4uVO1bsa/wGOUG6vPdNXCIK+uKZ16RncNhZxwhM1RDY/mi2ZDz9RAUwGXjnuXoSOWgaAR8R43yMk0TGxAK9VoirzQUNjQz7oFCCjJW1Gsu1+shKIDLskzAlPtAwzQWtm5JLnpE5aDojOCg3fbFmAtwhYmCiXugkIKMFfWay/V6CArg8sjdZ3TUVzuO5W+cn2xdb9vkIBdnEbYlDQXJJQ1HVnxu1/fPZ/6htRxz8CfMP7SW9f3zK+9fAtecFkHLP+6O3H1GR99843xu7TmtaLPYa63Y5CxzdRZhW9KQX9mEmjroef3wilZlnNsgZ2XTC3dx6jPfLf+YE7jmouZIZLHcoFIeuarnHktd79G4bq49y4TZ1BfXKY2W6XnDf33aEOfWr+77h2q2ck39jTRw6HBj2Fr3ln3PpTcvKIz6I/tIlLKpmnruqdRasUlSsGkW4Tql0UARlir0c0peMaat2LBDYaZwz5WjSz9B19ycMzOR5JKItLl/w/d5ccU7GLhqAi+ueAf3b/h+1G5WPbky7qnoiDYVx3Ig5M5ZIpxbv8FEo/iXajDdr4wegul3zb37Inj4p/GFb5ZB3JE292/4PnO3f41p7KFGYBp7mLv9a2rgI5Ir475oXhOrPnJCvOF0fsQZ8x0Fm2YReSPCufUbZHSZKb7bSmlDkKO09Jp76leZOfbjniHPfGANDVJcmK5Bepj5wJqK3k8p4K5DNYCqWkzBNsdknohwbv2clZtfP4lPyq+RIdbcGIqev0kYWS1DSS7uaqRHmT0+dzk4ysRXmK4ayZ1xrzrirmaoHCbCuS0dZLy4Yskw++Vr2CGcrJbhAiNxVyPdLVOZxh6f9ilMi9TT6mZU4y4iPwQ+COw2xsz12iYBtwCzgeeBxcaYV73XlgGfBvqBy4wxGxPpuaNoCFl1ElQe2VAyaA0rq2W46hXEO0N+4eSlTNj+tSJpptvU88IpS9W4RyCM5v4j4OySthZgszFmDrDZe46IHAdcCBzv7XO9iNTiMHFWzhsMIUs0yapasTyZ6yX8Nff9vLUy57xNjv2InPqhz/DoKd/kRaYyYIQXmcqjp3yTUz/0may75jSh4txFZDZw15CR+07gfcaYXSIyHfiNMeZYb9SOMWaVt91GYIUx5g8jvX9cce5xE3c8r1/8MxQcv9taYk67ryZKk7kgfAx5QpTO0E7xliocP2R0esDUs6z3Er7zv1aN8E6KEsxIce6Vau5HG2N2AXgG/iivvQn445DtOrw2v04tAZYAzJo1q8JuJEvcC0gkUawpqsyTC5koq0U5AvDLUO1iAfQW4t0b5WW6zGRW9y1m+9sWpt4/pTqI26Hq5yLynRoYY9YB66Awco+5H7EQtzGOu1hT1JrcZe0f87Jxsd5ULEvm8hsUGODOgQVs6Dlcr76hrpZVWstFSYhK49xf8uQYvMfdXnsHMHPIdjOArsq7ly6l+vrE8XW+21VqjONOsoqaKRh6/x1t9N3xhaKEmb47vlCxrh3ke7h/w/cr082zTuYKuaqUgeRzMBTFo9KR+wbgYqDVe7xjSPtPReRaoBGYA9wXtZNp4DeKrasR6mqF3v7DE4soxjjuELKoM4uw+x+4Zznj+w8WtY3pP1hor2D07ndTWdj/W+Y+cCMMpuiXUwQty8iRMlaVUt+KkiZhQiF/BrwPmCIiHcBVFIx6m4h8GvgL8DEAY8xjItIGPA70AZcaY/p93zgpKpQP/AxO74BhYkMdR4wdE5smHWcIWVSZJ+z+47pf9N0/qH00yqq9EkY3zzKZa4RVpTYcKpZgtJyukiajGndjzH8LeOmMgO2vBq6O0qmKiVACN2gUu7+7l4euOjPOXsZG1EzBsPt3DUxmRs3wOO2ugclUInz43VQCa6/s72BBmMVXIiZzVeoDMPs7fB1Ng6tK2eyotsmZblNf8kKuastEWdQglYqSMRO1lk7Y/W+o/zgHTH1R2wFTzw31H6+o336+h10BceBdZnLieQFR8g+C4tdfYgrbWk7nudZz2NZyunWGqqxjTjiHQPM/kiFf5QciRE3EXS8jLaLKPGH2P+mcJSy/vY/Lzc1vhvF9mwtZcM6Sij8Tin0PXcddQdMjVxXdnLsZyzW9xaPx2BdfIVrI66qej7HKJ359Ve/H+E5sPfQnymg39DGnsCBM3CHHSoF8GfcI9TbidnbmicI5+BwXbDwjQd/D6TD7yCLdvGXPucOckhD/Is5RHNPtb1tIy2vD49cnja/3FtlIxgcQNQw29DGnkEOgi3UnQ76Me8SoiUW121g0diWM64CxM6B2OeBeOncSpFJts0Q3b2/dAiks4hzFMb30rGNZeuvBovj1D4/ZxtfMf8B+L8LIwtFu6GNOIYdAF+tOhnxp7lHqbQxOPzNY/EDxJ61FnN//rqlltQ+jJAXvK7W3MKYkdDTuWutRR7uhz20KOQRLzzqWj9b/nq31l/Hs2IvYWn8ZH63/vfWSqO3ka+QOlUdNWJbCnntChKymJZXd++TwcrMjtQ9lzcad9A4UW/fpARUgbRrthj63KeQQLKrdxgfrbnjzhjhD9tJaewNjat+NzpwrJ3/GvVIsS2HPNWU46dKQg6KMgv226TJTmOEX2hnzaDdqAECoc5tGDsHmlcNmOmP6D+rAKiLVa9xLR44NRwasdq/rkcaOZbOkKKNgv31X9y3mmvobi5Oy4h7tphkAkPSCMDqwSoTqNO5+I8faeqipg4Hew9vpeqTJYNmPOcoo2G/fTbV/zydOns2pz3w30YzZsLMa6xOEMlxVKs9Up3H3Gzn290DDJKg/QtcjTRrLfsxRRsFB+54672wg+8UmooZMpkLGq0rlleo07kEjxO5X4crn0u1LNWLhjzlKGKzNi7I7kSCkC70nQnUad8tGjlWHbT/mFLIws8KZBCFd6D12qtO4WzhytJ64F+von8+aQ2vpOthN47gGlvYfC1lpwxk6eJPWwzVBqHqpTuNu28jRdmIe2frpwEtvfRgMb8aMp6oNZ+TgTUMPd7VmUtZY74QOQagFspPG1gWyg8jDF18W1831l7EqdEAHLRTuRyoLXAQd34SZ8MVHE/vYchZMj3LNVd31GpHSmy54SyJauHJWEgtkVy1ORB/ETaAD+pXDuQHeaP7+51/l8sfnjGhIytF7U9GGM5LpwurhUa85mx2+NuKEEzoE+aotUw4V1qiOum6pk4R1NPd207h99ah1ucvRe323jbu+eJSaRBEIu4ZAVV5zGeKME3oUqtO4RygSlpcvvizOWF4YyYZgOi8XPfczQn5Fq+pqhbqa4jWNfLXhpAq8nbi4IMGs2Fd4TMH/ErZ4V9bXXOnC8XlfRMPFhXv8qE7jXs6KTSWjxIvf4r/et2tffFn4jWwbJvlu2mUmD28rMUJ+K0Ct+ei7ueC0mdRKwcDXinD+KT5yQoTVtmxj0bwmzj+ladRjztLYVOMqSWlVI02a6tTcw0ZH+ESJfK32P3i9vo9be9775mYufvFlUxqHXHpuKKyctLpv+IjXzwiV6sDrH+zk4AM389u6m2mUvXSZKXz7gQtZ//ZJxcbOstIFUVj/YCe3be+k3wtq6DeG27Z30lxyzFlGvORFfy6HvCzcU53GPWwSk88ocUz/QVYecRt/GB/fqkRRyCwS4sTF3P/8q8x8YA1Hmb3slin8btb/YNNzx8NA+UboobvXsVLWvblc3QzZy0qzjtV3j2HRvK8f3jBHCWhrNu5kYf9vuaK+7c0b2uq+xazZWF/0HWZpbLKWhLIiD07o6jTuYaMjAkaD47tfZNuKhMPzQpBl5M76BztZdv/b6e49vFJow3O1nH9KE/c+uadsI3RJz48ZX9NT1DZeerik58fAEOOeowS05tc2Fa2/OkP20lp3A8teAyi+vrIyNpoE5S7VqbmHjY5IYRWaKGQZRRH02fc+uYdtLafzXOs5bGs5PbRBaqx52be9qWZvcWQMZBLZkgTL6n9etLA2FG5oy+p/nlGPhpMX/bkaqc6RO4SrZWH5KDHLKXPkzy4pZ9BbN4Gxvfv8tx2UYQYjY85dm2hyUVocHbBiU1B7FuRFf65Gqte4h8HyMgVZTpkjfbaPo3psTR39UketOVxP3wBSum+Olj6UAP+BWDIzHCQP+nM1Up2yTDlkEP8cllimzBUmBEX6bL9wxoFease9tUhuGWbYB3EwMsYXv/wBi2aGitvoyN1hIk+ZyygI5heVs+ojJ1T22WHr6QfUfDnQMI2FrVvclwksnxkqbhOpcJiIPA/8FegH+owxzSIyCbgFmA08Dyw2xrw60vu4VjgsN4QsmBV7IaWwhbp8Yun7asfR0nvJsDwDG4s6KUrSjFQ4LA5Z5v3GmJOGfEALsNkYMwfY7D1X0iaM3BIyISj2qJywcoRPVNM35bNFhj1yXxQlpyQhy5wHvM/7/ybgN8CVCXyOEsSONvru+AJj+g8Wnu9/ofAciqf8IROCYo/KKUeOKIlquqnl7nj74qFlcZW8EXXkboBfich2EVnitR1tjNkF4D0eFfEzlDI5cM/yw4bdY0z/QQ7cUzIyDjmCTqS2SYWO6iT6Uo31U5T8E9W4zzfGnAz8I3CpiPxd2B1FZImItItI+549eyJ2QxnKuO4Xw7WHTOayKZElib5oSV0lj0SSZYwxXd7jbhG5HTgNeElEphtjdonIdGB3wL7rgHVQcKhG6YdSTNfAZGbUDE+E6RqYzLAI6hDJXDYlsiTRl2qtn6Lkm4qNu4gcAdQYY/7q/X8msBLYAFwMtHqPd8TRUSU8N9R/nCt6ry9KbT9g6rmh/uOsqPA9bUpkKacvYbR0rZ+i5JEosszRwFYReRi4D7jbGPNfFIz6QhF5CljoPVdS5KRzlrDcLKFjYAoDRugYmMJys4STzlky+s45IqyWbpPspChxUfHI3RjzLPBun/aXgTOidEqJRmFk+jku2GhHWeKsCFuL3CbZSVHiQjNUc4pNMkpWlKOl6/lS8oYadyUT0ogrVy1dqWa0cJiSOmnFlauWrlQzatyV1EkrrtxvIW6tQaNUCyrLKKmTZly5aulKtaIjdyV1EilnoChKEWrcldRRLVxRkkdlGSV1NK5cUZJHjbuSCaqFK0qyqCyjKIqSQ9S4K4qi5BA17oqiKDlEjbuiKEoOUeOuKIqSQ8SY7BdBEpE9wJ9T+KgpwPAliqobPSf+6HnxR8+LP1mdl7cbY6b6vWCFcU8LEWk3xjRn3Q+b0HPij54Xf/S8+GPjeVFZRlEUJYeocVcURckh1Wbc12XdAQvRc+KPnhd/9Lz4Y915qSrNXVEUpVqotpG7oihKVaDGXVEUJYfkyriLyCQR2SQiT3mPRwZs90MR2S0ij1ayv2uUcV7OFpGdIvK0iLQMaV8hIp0i8pD394H0eh8/Qcc55HURkbXe6ztE5OSw+7pMxPPyvIg84l0f7en2PDlCnJN3icgfROSQiHylnH0TxxiTmz9gNdDi/d8CXBOw3d8BJwOPVrK/a39hjguoBZ4B/gaoBx4GjvNeWwF8JevjiOlcBB7nkG0+ANwDCPAe4E9h93X1L8p58V57HpiS9XFkcE6OAk4Frh76G7HhWsnVyB04D7jJ+/8mYJHfRsaY3wGvVLq/g4Q5rtOAp40xzxpjeoCbvf3yRpjjPA/4T1Pgj8BEEZkecl9XiXJe8sqo58QYs9sYcz/QW+6+SZM34360MWYXgPd4VMr720qY42oCXhjyvMNrG+Tz3lT8h47LVaMd50jbhNnXVaKcFwAD/EpEtovIksR6mS5Rvu/MrxXnVmISkV8D03xe+mrafbGJGM6L+LQNxsl+D/iG9/wbwLeAfy63j5Yw0nGOtk2YfV0lynkBmG+M6RKRo4BNIvKkN0N2mSjfd+bXinPG3RjzD0GvichLIjLdGLPLmy7uLvPto+6fGTGclw5g5pDnM4Au771fGvJePwDuiqfXmRB4nCG2qQ+xr6tEOS8YYwYfd4vI7RRkCdeNe5hzksS+sZA3WWYDcLH3/8XAHSnvbythjut+YI6IHCMi9cCF3n6U6KofBh712d8VAo9zCBuAT3rRIe8B9ntyVph9XaXi8yIiR4jIWwFE5AjgTNy+RgaJ8n1nf61k7ZGO2bs9GdgMPOU9TvLaG4FfDtnuZ8AuCk6QDuDTI+3v+l8Z5+UDwP+j4OX/6pD2/wM8AuygcIFOz/qYIp6PYccJfBb4rPe/AP/uvf4I0DzaOcrDX6XnhUJEyMPe32N5Oi8hzsk0z4a8Buzz/n+bDdeKlh9QFEXJIXmTZRRFURTUuCuKouQSNe6Koig5RI27oihKDlHjriiKkkPUuCuKouQQNe6Koig55P8D1ArfA+mgyqIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_test[:, 0], y_test)\n",
    "plt.scatter(X_test[:, 0], prediction)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "88ae486d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score is 0.45536123522262273 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error,r2_score,precision_score\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test,prediction))\n",
    "r2 = r2_score(y_test,prediction)\n",
    "\n",
    "\n",
    "print(\"r2 score is {} \".format(r2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a884378e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769cb0da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb752658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "01d6bd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - age     age in years\n",
      "      - sex\n",
      "      - bmi     body mass index\n",
      "      - bp      average blood pressure\n",
      "      - s1      tc, total serum cholesterol\n",
      "      - s2      ldl, low-density lipoproteins\n",
      "      - s3      hdl, high-density lipoproteins\n",
      "      - s4      tch, total cholesterol / HDL\n",
      "      - s5      ltg, possibly log of serum triglycerides level\n",
      "      - s6      glu, blood sugar level\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "print(diabetes.data.shape)\n",
    "print(diabetes.DESCR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
